{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB \n",
    "# ChromaDB is an AI native open source vector data base focused on developer productivity  and happiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter \n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (SentenceTransformerEmbeddings)\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('sample.txt',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x269d3bf8a30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Customizing PaliGemma: A Guide to Finetuning for Targeted Applications\\nPaligemma : \\nPaligemma is a sophisticated, integrated AI system that combines vision and language models to provide comprehensive multimodal understanding and generation capabilities. The name \"Paligemma\" suggests a combination of \"Pali,\" potentially hinting at a foundational or structural aspect, and \"gemma,\" which can imply something valuable or precious, indicating the integration of crucial AI components. PaliGemma is a vision-language model (VLM) developed by Google. It is a multimodal model that combines the capabilities of a vision model and a language model. The model is composed of a Siglip-400m vision encoder and a Gemma-2B decoder linked by a multimodal linear projection. PaliGemma is designed to process both images and text and generate text as output, supporting multiple languages\\n\\nKey Features and Capabilities\\n1.\\tMultimodal Comprehension: PaliGemma can simultaneously understand both images and text, making it suitable for tasks such as image captioning, visual question answering, and text reading from images.\\n2.\\tFine-Tuning: PaliGemma is designed to be fine-tuned on specific tasks, which allows it to adapt to different use cases and achieve better performance. This fine-tuning process involves adjusting the model\\'s weights based on the specific task and dataset.\\n3.\\tPre-Training: PaliGemma is pre-trained on a variety of datasets, including WebLI, CC3M-35L, VQ²A-CC3M-35L/VQG-CC3M-35L, OpenImages, and WIT. This pre-training helps the model learn general representations of images and text that can be leveraged for downstream tasks.\\n4.\\tResolutions and Precisions: PaliGemma models come in three resolutions (224x224, 448x448, and 896x896) and three precisions (bfloat16, float16, and float32). The higher resolutions are more memory-intensive but can be beneficial for fine-grained tasks like optical character recognition (OCR).\\n5.\\tIntegration with Transformers: PaliGemma models are integrated with the transformers library, making it easy to use and fine-tune the models for specific tasks\\nUse Cases and Benchmarks\\nPaliGemma is suitable for a variety of tasks, including:\\n1.\\tImage Captioning: PaliGemma can generate captions for images based on the input text and image.\\n2.\\tVisual Question Answering: The model can answer questions about images, providing detailed and contextual responses.\\n3.\\tText Reading from Images: PaliGemma can read text embedded within images, such as captions or signs.\\n4.\\tObject Detection and Segmentation: The model can be fine-tuned for tasks like object detection and segmentation, which involve identifying and localizing objects within images.\\nLimitations and Future Directions\\n1.\\tNiche Datasets: PaliGemma may struggle with niche datasets or environments that were not present during pretraining, which is expected given the limited scope of its pretraining.\\n2.\\tFine-Tuning: While PaliGemma is designed to be fine-tuned, the model\\'s performance can be improved significantly by fine-tuning it on specific tasks and datasets.\\n3.\\tComparison to Other Models: PaliGemma can be compared to other VLMs and LMMs, such as ChatGPT-4o, which have larger architectures but may not be as efficient or fine-tunable\\n\\nArchitecture of Paligemma\\nThe architecture of Paligemma can be divided into several key components:\\n1.\\tInput Processing Module:\\n○\\tVision Processing: This module processes visual inputs using advanced vision models such as SigLIP.\\n○\\tLanguage Processing: This module handles textual inputs using the Gemma language model.\\n2.\\tMultimodal Fusion Layer:\\n○\\tThis layer integrates outputs from both the vision and language processing modules to create a unified representation. Techniques like cross-modal attention mechanisms are often used here.\\n3.\\tCore Understanding Engine:\\n○\\tContextual Understanding: Integrates multimodal information to understand the context and nuances of the input data.\\n○\\tKnowledge Integration: Utilizes external knowledge bases to enhance understanding and provide more accurate responses.\\n4.\\tOutput Generation Module:\\n○\\tResponse Generation: Uses the integrated representation to generate appropriate responses or actions.\\n○\\tAdaptive Learning: Continuously learns from interactions to improve future responses.\\n5.\\tFeedback Loop:\\n○\\tPerformance Monitoring: Tracks the performance of the system and identifies areas for improvement.\\n○\\tIterative Learning: Updates the model based on feedback to refine its capabilities.\\nPaliGemma is a vision-language model (VLM) developed by Google that combines a vision encoder and a language decoder. Its architecture consists of:\\n●\\tSigLIP-400m as the vision encoder: SigLIP is a robust contrastively trained visual encoder similar to OpenAI\\'s CLIP, but using a simpler sigmoid loss function.\\n●\\tGemma-2B as the text decoder: Gemma is a relatively compact decoder-only language model from Google. It tokenizes the input text and processes all tokens using its 256,000 token vocabulary.\\n●\\tGemma\\'s transformer-based decoder: The decoder is largely similar to the original transformer decoder by Vaswani et al. (2017), with modifications like multi-head attention, rotary positional embeddings, GeGLU activation, and RMSNorm.\\n●\\tAdditional tokens: PaliGemma extends Gemma\\'s token vocabulary with 1024 location tokens (<loc0000> to <loc1023>) representing normalized image coordinates, and 128 segmentation tokens (<seg000> to <seg127>) from a vector quantized visual auto-encoder.\\nThe vision encoder and language decoder are linked using a multimodal linear projection. PaliGemma is designed to take both image and text as input and generate text as output, supporting multiple languages.\\nThe model has a total of 3 billion parameters and is pre-trained on a mixture of datasets like WebLI, CC3M-35L, VQ²A-CC3M-35L/VQG-CC3M-35L, OpenImages, and WIT. It is designed to be fine-tuned on specific vision-language tasks for better performance\\n\\nHow Paligemma Works\\n1.\\tInput Reception: The system receives visual and textual inputs.\\n2.\\tProcessing: The inputs are processed through their respective modules—visual data through SigLIP and textual data through Gemma.\\n3.\\tIntegration: The multimodal fusion layer combines the processed data into a coherent representation.\\n4.\\tUnderstanding: The core understanding engine interprets the integrated data, using context and external knowledge.\\n5.\\tResponse Generation: An appropriate response is generated based on the interpretation.\\n6.\\tLearning and Adaptation: The system learns from interactions and feedback to improve its future performance.\\n\\n \\nSigLIP Vision Model\\nDefinition\\nSigLIP (Signal Language-Image Pretraining) is a vision model designed to understand and interpret visual data. It employs a pretraining technique that integrates both visual and textual information to enhance its understanding capabilities.\\nSigLIP is based on the Vision Transformer (ViT) architecture, which uses self-attention mechanisms to process input images. The model consists of a series of transformer blocks, each of which includes a multi-head self-attention mechanism and a feed-forward network (FFN). The output of each block is a set of feature maps that capture different aspects of the input image\\nArchitecture of SigLIP\\n1.\\tImage Encoder:\\n○\\tUtilizes Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs) to extract features from images.\\n○\\tMulti-layered structure with attention mechanisms to focus on important aspects of the visual data.\\n2.\\tText Encoder:\\n○\\tIncorporates a language model (like BERT or GPT) to process textual descriptions associated with images.\\n○\\tEmbedding layers to convert text into vector representations.\\n3.\\tCross-Modal Attention Mechanism:\\n○\\tConnects the image and text encoders, allowing the model to learn correspondences between visual features and textual descriptions.\\n○\\tUses attention layers to highlight relevant parts of the image based on the text and vice versa.\\n4.\\tFusion Layer:\\n○\\tCombines the outputs of the image and text encoders into a unified representation.\\n○\\tDense layers and normalization techniques to ensure cohesive integration.\\n5.\\tOutput Layer:\\n○\\tProduces predictions or classifications based on the fused representation.\\n○\\tCan be fine-tuned for specific tasks such as image captioning, visual question answering, or object recognition.\\nFunctionality\\n●\\tPretraining: The model is pretrained on large datasets containing paired image-text data to learn the relationships between visual and textual information.\\n●\\tFine-Tuning: After pretraining, the model can be fine-tuned on specific datasets to adapt to various vision-related tasks.\\nTraining\\nSigLIP is trained using a contrastive loss function, which aims to maximize the similarity between positive pairs of images and minimize the similarity between negative pairs. This approach helps the model learn robust and generalizable representations of images. The model is trained on a large dataset of images and their corresponding text descriptions, which are used to generate positive and negative pairs.\\n \\nKey Features\\n1.\\tRobustness: SigLIP is designed to be robust to various types of image corruptions and transformations, such as noise, blur, and rotation. This makes it suitable for real-world applications where images may be degraded or distorted.\\n2.\\tEfficiency: SigLIP is optimized for efficiency and can be used on a wide range of devices, from mobile phones to high-performance servers. This makes it a practical choice for many applications.\\n3.\\tMultimodal Capabilities: SigLIP can be used in conjunction with other models, such as language models, to perform multimodal tasks like image captioning and visual question answering.\\nComparison to Other Models\\nSigLIP is comparable to other robust visual encoders like CLIP, which is also developed by OpenAI. While both models are designed to be robust and efficient, SigLIP is simpler and more lightweight, making it easier to integrate into various applications.\\n\\n\\n\\n\\n\\nImplementation\\nSigLIP can be implemented using the transformers library in Python. The following code snippet demonstrates how to use SigLIP for image captioning:\\n \\nGemma Language Model\\nDefinition\\nThe Gemma language model is an advanced neural network model designed to understand and generate human language. It leverages extensive pretraining on vast amounts of text data to develop a deep understanding of language nuances. \\nGemma is a family of lightweight, state-of-the-art open models developed by Google. It is designed to be a robust and efficient model that can be used for various applications such as text generation and multimodal tasks. Here are the key aspects of Gemma:\\nArchitecture and Training\\nGemma is based on the transformer architecture and is trained using a combination of masked language modeling and next sentence prediction tasks. The model is trained on a large dataset of text and is designed to be robust and efficient.\\nModel Sizes and Capabilities\\nGemma models are available in two sizes: 2B and 7B. The 2B model is designed for lower resource requirements and can run on mobile devices and laptops, while the 7B model is more powerful and can run on desktop computers and small servers.\\nTuning and Customization\\nGemma models can be tuned and customized for specific tasks using techniques such as LoRA (Low-Rank Adaptation) and model parallelism. This allows developers to adapt the model to their specific needs and improve its performance on targeted tasks.\\nResponsible AI Development\\nGemma is designed with responsible AI development in mind. The model is trained on curated data and is tuned for safety using techniques such as automated filtering of personal information and extensive fine-tuning with human feedback. The model is also evaluated using robust methods such as manual red-teaming and automated adversarial testing to ensure it does not exhibit dangerous behaviors.\\nDeployment and Integration\\nGemma models can be deployed on various platforms, including Google Cloud, and can be integrated with popular frameworks such as JAX, PyTorch, and TensorFlow. The model can also be fine-tuned on specific data sets and tasks using tools such as LoRA and model parallelism.\\nPerformance and Benchmarks\\nGemma models have achieved state-of-the-art performance for their size compared to other open models. The model has been tested on various benchmarks and has shown exceptional performance in tasks such as text generation and multimodal tasks.\\nAvailability and Community\\nGemma models are available for download from Kaggle and can be used for various applications. The model has a growing community of developers and researchers who are working on fine-tuning and customizing the model for specific tasks.\\nKey Features\\n1.\\tLightweight and Efficient: Gemma models are designed to be lightweight and efficient, making them suitable for deployment on a wide range of devices and platforms.\\n2.\\tState-of-the-Art Performance: Gemma models have achieved state-of-the-art performance for their size compared to other open models.\\n3.\\tResponsible AI Development: Gemma is designed with responsible AI development in mind, incorporating techniques such as automated filtering of personal information and extensive fine-tuning with human feedback.\\n4.\\tCustomization and Tuning: Gemma models can be tuned and customized for specific tasks using techniques such as LoRA and model parallelism.\\n5.\\tIntegration with Popular Frameworks: Gemma models can be integrated with popular frameworks such as JAX, PyTorch, and TensorFlow, making it easy to use and deploy\\n\\nArchitecture of Gemma\\n1.\\tEmbedding Layer:\\n○\\tConverts input text into dense vector representations.\\n○\\tUtilizes word embeddings or contextual embeddings like those from BERT or GPT.\\n2.\\tTransformer Layers:\\n○\\tMultiple transformer blocks, each containing self-attention mechanisms and feed-forward neural networks.\\n○\\tLayer normalization and residual connections to maintain stable training.\\n3.\\tContextual Understanding Module:\\n○\\tEncodes the context of the input text to generate coherent and contextually appropriate responses.\\n○\\tUses attention mechanisms to focus on relevant parts of the input text.\\n4.\\tOutput Layer:\\n○\\tGenerates the final output text based on the processed and contextually understood input.\\n○\\tCan produce various forms of output such as summaries, translations, or conversational responses.\\nFunctionality\\n●\\tPretraining: Trained on diverse text corpora to learn language patterns, grammar, and context.\\n●\\tFine-Tuning: Adapted to specific tasks like question answering, text generation, or sentiment analysis through fine-tuning on relevant datasets.\\nIntegration in Paligemma\\nIn the Paligemma system, SigLIP and Gemma work in tandem to process and understand multimodal inputs. SigLIP handles visual data, converting it into a format that can be integrated with the textual data processed by Gemma. The fusion layer then combines these representations, allowing the core understanding engine to interpret the integrated data and generate appropriate responses. This integration enables Paligemma to excel in tasks that require a deep understanding of both visual and textual information.\\nModel Data\\nData for Paligemma\\nPaligemma, being a multimodal system, requires diverse datasets encompassing both visual and textual data:\\n1.\\tImage-Text Pairs: Datasets like COCO (Common Objects in Context) and Visual Genome provide images with corresponding textual descriptions or annotations.\\n2.\\tTextual Data: Large corpora of text, such as Wikipedia articles, books, and web pages, are used to pretrain the language model (Gemma).\\n3.\\tAnnotated Multimodal Data: Datasets specifically designed for tasks like visual question answering (VQA), image captioning, and scene understanding, which combine both image and text annotations.\\nData for SigLIP\\nSigLIP\\'s training data includes:\\n1.\\tImage Datasets: High-quality and diverse image datasets like ImageNet, COCO, and Open Images.\\n2.\\tPaired Image-Text Data: Data where each image is paired with descriptive text, aiding the model in learning the relationships between visual content and language.\\nData for Gemma\\nGemma\\'s data requirements are primarily textual:\\n1.\\tLarge-Scale Text Corpora: Datasets such as the Common Crawl, Wikipedia, and BookCorpus provide extensive text data for pretraining.\\n2.\\tSpecialized Text Datasets: Fine-tuning datasets tailored to specific tasks like sentiment analysis, question answering (SQuAD), and natural language inference.\\nModel Building\\nBuilding Paligemma\\n1.\\tData Collection and Preprocessing:\\n○\\tCollect and clean large volumes of multimodal data.\\n○\\tPreprocess images (resizing, normalization) and text (tokenization, normalization).\\n2.\\tPretraining:\\n○\\tTrain SigLIP on image-text pairs to learn cross-modal representations.\\n○\\tTrain Gemma on large-scale text corpora to develop a deep understanding of language.\\n3.\\tMultimodal Fusion:\\n○\\tDevelop and train the fusion layer to integrate visual and textual features.\\n○\\tUtilize techniques like cross-attention to effectively combine modalities.\\n4.\\tFine-Tuning:\\n○\\tFine-tune the integrated model on task-specific multimodal datasets.\\n○\\tUse transfer learning to adapt pre-trained models to new tasks with limited data.\\nBuilding SigLIP\\n1.\\tImage Encoder Training:\\n○\\tUse convolutional neural networks (CNNs) or Vision Transformers (ViTs) to extract features from images.\\n○\\tTrain on large image datasets to develop robust visual representations.\\n2.\\tText Encoder Training:\\n○\\tEmploy transformers to process textual descriptions associated with images.\\n○\\tTrain on paired image-text data to learn the correlation between visual and textual information.\\n3.\\tCross-Modal Pretraining:\\n○\\tImplement cross-modal attention mechanisms to align image features with textual features.\\n○\\tPretrain on large-scale datasets to learn rich, shared representations.\\nBuilding Gemma\\n1.\\tEmbedding Layer Setup:\\n○\\tInitialize word embeddings using pre-trained vectors or train embeddings from scratch on a large text corpus.\\n2.\\tTransformer Training:\\n○\\tUse transformer architecture with multiple layers of self-attention and feed-forward networks.\\n○\\tPretrain on extensive text data to capture language patterns and contextual relationships.\\n3.\\tContextual Understanding:\\n○\\tIntegrate advanced attention mechanisms to focus on relevant text segments.\\n○\\tTrain on datasets like books, articles, and dialogues to understand various contexts and nuances.\\nModel Architecture\\nArchitecture of Paligemma\\n1.\\tInput Processing Module:\\n○\\tSigLIP Vision Processing: Image encoder + text encoder + cross-modal attention.\\n○\\tGemma Language Processing: Transformer-based language model.\\n2.\\tMultimodal Fusion Layer:\\n○\\tCross-attention mechanisms to combine image and text features.\\n○\\tDense layers and normalization to create a unified representation.\\n3.\\tCore Understanding Engine:\\n○\\tContextual understanding through integrated representations.\\n○\\tExternal knowledge integration via APIs or databases to enhance comprehension.\\n4.\\tOutput Generation Module:\\n○\\tDecoders for generating textual responses or actions.\\n○\\tTask-specific layers for applications like VQA or image captioning.\\n5.\\tFeedback Loop:\\n○\\tPerformance monitoring to identify strengths and weaknesses.\\n○\\tIterative learning to adapt and improve based on feedback.\\nArchitecture of SigLIP\\n1.\\tImage Encoder:\\n○\\tConvolutional layers or Vision Transformer layers to process images.\\n○\\tAttention mechanisms to focus on important visual features.\\n2.\\tText Encoder:\\n○\\tTransformer layers to process associated textual descriptions.\\n○\\tEmbedding layers to convert text into meaningful vectors.\\n3.\\tCross-Modal Attention Mechanism:\\n○\\tAttention layers connecting image and text encoders.\\n○\\tMechanisms to highlight relevant image regions based on text and vice versa.\\n4.\\tFusion Layer:\\n○\\tCombines image and text embeddings into a cohesive representation.\\n○\\tDense layers for integration and normalization.\\n5.\\tOutput Layer:\\n○\\tTask-specific layers for classification, captioning, or other vision tasks.\\n○\\tFine-tuning mechanisms to adapt to different applications.\\nArchitecture of Gemma\\n1.\\tEmbedding Layer:\\n○\\tConverts text into dense vector representations.\\n○\\tUtilizes pre-trained word embeddings or contextual embeddings.\\n2.\\tTransformer Layers:\\n○\\tMultiple layers of self-attention and feed-forward networks.\\n○\\tLayer normalization and residual connections for stable training.\\n3.\\tContextual Understanding Module:\\n○\\tAttention mechanisms to focus on relevant parts of the text.\\n○\\tMechanisms to maintain context over long text sequences.\\n4.\\tOutput Layer:\\n○\\tGenerates final text outputs, such as responses, summaries, or translations.\\n○\\tTask-specific adaptations for different language applications.\\nBy integrating these sophisticated components, Paligemma achieves a powerful synergy between vision and language, enabling it to perform complex tasks that require deep understanding and generation capabilities across both modalities.\\nFine-tuning Paligemma involves adjusting the pre-trained models (SigLIP for vision and Gemma for language) on a specific task using a tailored dataset. Here\\'s a detailed guide on how to fine-tune Paligemma, along with code examples.\\nPrerequisites\\n1.\\tData Preparation: You need a dataset relevant to your task, which contains both visual and textual information. For example, a Visual Question Answering (VQA) dataset.\\n2.\\tEnvironment Setup: Ensure you have a suitable machine learning environment with necessary libraries installed (e.g., PyTorch, Transformers, Vision libraries).\\nStep-by-Step Fine-Tuning Process\\n \\nPaliGemma is a vision-language model (VLM) developed by Google that can be fine-tuned for various tasks. Fine-tuning involves adjusting the model\\'s weights based on specific task requirements and datasets to improve its performance. Here\\'s a step-by-step guide on how to fine-tune PaliGemma:\\nStep 1: Download the Model and Dependencies\\n1.\\tDownload PaliGemma Model Checkpoint: Download the pre-trained PaliGemma model checkpoint and tokenizer from Kaggle or other sources.\\n2.\\tInstall Dependencies: Install the required dependencies, including JAX, TensorFlow, NumPy, and other libraries.\\nStep 2: Prepare the Model\\n1.\\tLoad the Model: Load the pre-trained PaliGemma model onto GPU devices.\\n2.\\tPrepare Inputs: Prepare the model\\'s inputs for training and inference by processing the data into the required format.\\nStep 3: Fine-tune the Model\\n1.\\tFreeze Parameters: Freeze the majority of the model\\'s parameters, except for the attention layers, to prevent overfitting.\\n2.\\tTrain the Model: Train the fine-tuned model using the specific task\\'s dataset and hyperparameters. This step can be done using JAX and other libraries.\\nStep 4: Test and Save the Model\\n1.\\tTest the Model: Test the fine-tuned model on a validation dataset to evaluate its performance.\\n2.\\tSave the Model: Save the fine-tuned model for later use by saving its weights and other parameters.\\nAdditional Tips\\n1.\\tUse a Small Version: For fine-tuning in Google Colab, use the smallest version of PaliGemma (paligemma-3b-pt-224) to limit GPU memory consumption.\\n2.\\tUse Roboflow Universe: Use Roboflow Universe for accessing and managing datasets for fine-tuning PaliGemma.\\n3.\\tFine-tune for Specific Tasks: Fine-tune PaliGemma for specific tasks such as object detection, segmentation, or text reading from images\\n\\n1. Import Necessary Libraries\\n \\n\\n2. Prepare the Dataset\\nDefine a custom dataset class to handle your data.\\n \\n\\n3. Initialize Pre-trained Models\\nLoad pre-trained models for vision and language.\\n \\n4. Define the Multimodal Model\\nCombine vision and language models into a single architecture.\\n \\n\\n5. Prepare for Training\\nSet up the data loaders, loss function, and optimizer.\\n \\n6. Fine-Tuning the Model\\nDefine the training loop to fine-tune the model.\\n \\n\\nCustomizing PaliGemma: A Guide to Finetuning for Targeted Applications\\nPaligemma : \\nPaligemma is a sophisticated, integrated AI system that combines vision and language models to provide comprehensive multimodal understanding and generation capabilities. The name \"Paligemma\" suggests a combination of \"Pali,\" potentially hinting at a foundational or structural aspect, and \"gemma,\" which can imply something valuable or precious, indicating the integration of crucial AI components. PaliGemma is a vision-language model (VLM) developed by Google. It is a multimodal model that combines the capabilities of a vision model and a language model. The model is composed of a Siglip-400m vision encoder and a Gemma-2B decoder linked by a multimodal linear projection. PaliGemma is designed to process both images and text and generate text as output, supporting multiple languages\\n\\nKey Features and Capabilities\\n1.\\tMultimodal Comprehension: PaliGemma can simultaneously understand both images and text, making it suitable for tasks such as image captioning, visual question answering, and text reading from images.\\n2.\\tFine-Tuning: PaliGemma is designed to be fine-tuned on specific tasks, which allows it to adapt to different use cases and achieve better performance. This fine-tuning process involves adjusting the model\\'s weights based on the specific task and dataset.\\n3.\\tPre-Training: PaliGemma is pre-trained on a variety of datasets, including WebLI, CC3M-35L, VQ²A-CC3M-35L/VQG-CC3M-35L, OpenImages, and WIT. This pre-training helps the model learn general representations of images and text that can be leveraged for downstream tasks.\\n4.\\tResolutions and Precisions: PaliGemma models come in three resolutions (224x224, 448x448, and 896x896) and three precisions (bfloat16, float16, and float32). The higher resolutions are more memory-intensive but can be beneficial for fine-grained tasks like optical character recognition (OCR).\\n5.\\tIntegration with Transformers: PaliGemma models are integrated with the transformers library, making it easy to use and fine-tune the models for specific tasks\\nUse Cases and Benchmarks\\nPaliGemma is suitable for a variety of tasks, including:\\n1.\\tImage Captioning: PaliGemma can generate captions for images based on the input text and image.\\n2.\\tVisual Question Answering: The model can answer questions about images, providing detailed and contextual responses.\\n3.\\tText Reading from Images: PaliGemma can read text embedded within images, such as captions or signs.\\n4.\\tObject Detection and Segmentation: The model can be fine-tuned for tasks like object detection and segmentation, which involve identifying and localizing objects within images.\\nLimitations and Future Directions\\n1.\\tNiche Datasets: PaliGemma may struggle with niche datasets or environments that were not present during pretraining, which is expected given the limited scope of its pretraining.\\n2.\\tFine-Tuning: While PaliGemma is designed to be fine-tuned, the model\\'s performance can be improved significantly by fine-tuning it on specific tasks and datasets.\\n3.\\tComparison to Other Models: PaliGemma can be compared to other VLMs and LMMs, such as ChatGPT-4o, which have larger architectures but may not be as efficient or fine-tunable\\n\\nArchitecture of Paligemma\\nThe architecture of Paligemma can be divided into several key components:\\n1.\\tInput Processing Module:\\n○\\tVision Processing: This module processes visual inputs using advanced vision models such as SigLIP.\\n○\\tLanguage Processing: This module handles textual inputs using the Gemma language model.\\n2.\\tMultimodal Fusion Layer:\\n○\\tThis layer integrates outputs from both the vision and language processing modules to create a unified representation. Techniques like cross-modal attention mechanisms are often used here.\\n3.\\tCore Understanding Engine:\\n○\\tContextual Understanding: Integrates multimodal information to understand the context and nuances of the input data.\\n○\\tKnowledge Integration: Utilizes external knowledge bases to enhance understanding and provide more accurate responses.\\n4.\\tOutput Generation Module:\\n○\\tResponse Generation: Uses the integrated representation to generate appropriate responses or actions.\\n○\\tAdaptive Learning: Continuously learns from interactions to improve future responses.\\n5.\\tFeedback Loop:\\n○\\tPerformance Monitoring: Tracks the performance of the system and identifies areas for improvement.\\n○\\tIterative Learning: Updates the model based on feedback to refine its capabilities.\\nPaliGemma is a vision-language model (VLM) developed by Google that combines a vision encoder and a language decoder. Its architecture consists of:\\n●\\tSigLIP-400m as the vision encoder: SigLIP is a robust contrastively trained visual encoder similar to OpenAI\\'s CLIP, but using a simpler sigmoid loss function.\\n●\\tGemma-2B as the text decoder: Gemma is a relatively compact decoder-only language model from Google. It tokenizes the input text and processes all tokens using its 256,000 token vocabulary.\\n●\\tGemma\\'s transformer-based decoder: The decoder is largely similar to the original transformer decoder by Vaswani et al. (2017), with modifications like multi-head attention, rotary positional embeddings, GeGLU activation, and RMSNorm.\\n●\\tAdditional tokens: PaliGemma extends Gemma\\'s token vocabulary with 1024 location tokens (<loc0000> to <loc1023>) representing normalized image coordinates, and 128 segmentation tokens (<seg000> to <seg127>) from a vector quantized visual auto-encoder.\\nThe vision encoder and language decoder are linked using a multimodal linear projection. PaliGemma is designed to take both image and text as input and generate text as output, supporting multiple languages.\\nThe model has a total of 3 billion parameters and is pre-trained on a mixture of datasets like WebLI, CC3M-35L, VQ²A-CC3M-35L/VQG-CC3M-35L, OpenImages, and WIT. It is designed to be fine-tuned on specific vision-language tasks for better performance\\n\\nHow Paligemma Works\\n1.\\tInput Reception: The system receives visual and textual inputs.\\n2.\\tProcessing: The inputs are processed through their respective modules—visual data through SigLIP and textual data through Gemma.\\n3.\\tIntegration: The multimodal fusion layer combines the processed data into a coherent representation.\\n4.\\tUnderstanding: The core understanding engine interprets the integrated data, using context and external knowledge.\\n5.\\tResponse Generation: An appropriate response is generated based on the interpretation.\\n6.\\tLearning and Adaptation: The system learns from interactions and feedback to improve its future performance.\\n\\n \\nSigLIP Vision Model\\nDefinition\\nSigLIP (Signal Language-Image Pretraining) is a vision model designed to understand and interpret visual data. It employs a pretraining technique that integrates both visual and textual information to enhance its understanding capabilities.\\nSigLIP is based on the Vision Transformer (ViT) architecture, which uses self-attention mechanisms to process input images. The model consists of a series of transformer blocks, each of which includes a multi-head self-attention mechanism and a feed-forward network (FFN). The output of each block is a set of feature maps that capture different aspects of the input image\\nArchitecture of SigLIP\\n1.\\tImage Encoder:\\n○\\tUtilizes Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs) to extract features from images.\\n○\\tMulti-layered structure with attention mechanisms to focus on important aspects of the visual data.\\n2.\\tText Encoder:\\n○\\tIncorporates a language model (like BERT or GPT) to process textual descriptions associated with images.\\n○\\tEmbedding layers to convert text into vector representations.\\n3.\\tCross-Modal Attention Mechanism:\\n○\\tConnects the image and text encoders, allowing the model to learn correspondences between visual features and textual descriptions.\\n○\\tUses attention layers to highlight relevant parts of the image based on the text and vice versa.\\n4.\\tFusion Layer:\\n○\\tCombines the outputs of the image and text encoders into a unified representation.\\n○\\tDense layers and normalization techniques to ensure cohesive integration.\\n5.\\tOutput Layer:\\n○\\tProduces predictions or classifications based on the fused representation.\\n○\\tCan be fine-tuned for specific tasks such as image captioning, visual question answering, or object recognition.\\nFunctionality\\n●\\tPretraining: The model is pretrained on large datasets containing paired image-text data to learn the relationships between visual and textual information.\\n●\\tFine-Tuning: After pretraining, the model can be fine-tuned on specific datasets to adapt to various vision-related tasks.\\nTraining\\nSigLIP is trained using a contrastive loss function, which aims to maximize the similarity between positive pairs of images and minimize the similarity between negative pairs. This approach helps the model learn robust and generalizable representations of images. The model is trained on a large dataset of images and their corresponding text descriptions, which are used to generate positive and negative pairs.\\n \\nKey Features\\n1.\\tRobustness: SigLIP is designed to be robust to various types of image corruptions and transformations, such as noise, blur, and rotation. This makes it suitable for real-world applications where images may be degraded or distorted.\\n2.\\tEfficiency: SigLIP is optimized for efficiency and can be used on a wide range of devices, from mobile phones to high-performance servers. This makes it a practical choice for many applications.\\n3.\\tMultimodal Capabilities: SigLIP can be used in conjunction with other models, such as language models, to perform multimodal tasks like image captioning and visual question answering.\\nComparison to Other Models\\nSigLIP is comparable to other robust visual encoders like CLIP, which is also developed by OpenAI. While both models are designed to be robust and efficient, SigLIP is simpler and more lightweight, making it easier to integrate into various applications.\\n\\n\\n\\n\\n\\nImplementation\\nSigLIP can be implemented using the transformers library in Python. The following code snippet demonstrates how to use SigLIP for image captioning:\\n \\nGemma Language Model\\nDefinition\\nThe Gemma language model is an advanced neural network model designed to understand and generate human language. It leverages extensive pretraining on vast amounts of text data to develop a deep understanding of language nuances. \\nGemma is a family of lightweight, state-of-the-art open models developed by Google. It is designed to be a robust and efficient model that can be used for various applications such as text generation and multimodal tasks. Here are the key aspects of Gemma:\\nArchitecture and Training\\nGemma is based on the transformer architecture and is trained using a combination of masked language modeling and next sentence prediction tasks. The model is trained on a large dataset of text and is designed to be robust and efficient.\\nModel Sizes and Capabilities\\nGemma models are available in two sizes: 2B and 7B. The 2B model is designed for lower resource requirements and can run on mobile devices and laptops, while the 7B model is more powerful and can run on desktop computers and small servers.\\nTuning and Customization\\nGemma models can be tuned and customized for specific tasks using techniques such as LoRA (Low-Rank Adaptation) and model parallelism. This allows developers to adapt the model to their specific needs and improve its performance on targeted tasks.\\nResponsible AI Development\\nGemma is designed with responsible AI development in mind. The model is trained on curated data and is tuned for safety using techniques such as automated filtering of personal information and extensive fine-tuning with human feedback. The model is also evaluated using robust methods such as manual red-teaming and automated adversarial testing to ensure it does not exhibit dangerous behaviors.\\nDeployment and Integration\\nGemma models can be deployed on various platforms, including Google Cloud, and can be integrated with popular frameworks such as JAX, PyTorch, and TensorFlow. The model can also be fine-tuned on specific data sets and tasks using tools such as LoRA and model parallelism.\\nPerformance and Benchmarks\\nGemma models have achieved state-of-the-art performance for their size compared to other open models. The model has been tested on various benchmarks and has shown exceptional performance in tasks such as text generation and multimodal tasks.\\nAvailability and Community\\nGemma models are available for download from Kaggle and can be used for various applications. The model has a growing community of developers and researchers who are working on fine-tuning and customizing the model for specific tasks.\\nKey Features\\n1.\\tLightweight and Efficient: Gemma models are designed to be lightweight and efficient, making them suitable for deployment on a wide range of devices and platforms.\\n2.\\tState-of-the-Art Performance: Gemma models have achieved state-of-the-art performance for their size compared to other open models.\\n3.\\tResponsible AI Development: Gemma is designed with responsible AI development in mind, incorporating techniques such as automated filtering of personal information and extensive fine-tuning with human feedback.\\n4.\\tCustomization and Tuning: Gemma models can be tuned and customized for specific tasks using techniques such as LoRA and model parallelism.\\n5.\\tIntegration with Popular Frameworks: Gemma models can be integrated with popular frameworks such as JAX, PyTorch, and TensorFlow, making it easy to use and deploy\\n\\nArchitecture of Gemma\\n1.\\tEmbedding Layer:\\n○\\tConverts input text into dense vector representations.\\n○\\tUtilizes word embeddings or contextual embeddings like those from BERT or GPT.\\n2.\\tTransformer Layers:\\n○\\tMultiple transformer blocks, each containing self-attention mechanisms and feed-forward neural networks.\\n○\\tLayer normalization and residual connections to maintain stable training.\\n3.\\tContextual Understanding Module:\\n○\\tEncodes the context of the input text to generate coherent and contextually appropriate responses.\\n○\\tUses attention mechanisms to focus on relevant parts of the input text.\\n4.\\tOutput Layer:\\n○\\tGenerates the final output text based on the processed and contextually understood input.\\n○\\tCan produce various forms of output such as summaries, translations, or conversational responses.\\nFunctionality\\n●\\tPretraining: Trained on diverse text corpora to learn language patterns, grammar, and context.\\n●\\tFine-Tuning: Adapted to specific tasks like question answering, text generation, or sentiment analysis through fine-tuning on relevant datasets.\\nIntegration in Paligemma\\nIn the Paligemma system, SigLIP and Gemma work in tandem to process and understand multimodal inputs. SigLIP handles visual data, converting it into a format that can be integrated with the textual data processed by Gemma. The fusion layer then combines these representations, allowing the core understanding engine to interpret the integrated data and generate appropriate responses. This integration enables Paligemma to excel in tasks that require a deep understanding of both visual and textual information.\\nModel Data\\nData for Paligemma\\nPaligemma, being a multimodal system, requires diverse datasets encompassing both visual and textual data:\\n1.\\tImage-Text Pairs: Datasets like COCO (Common Objects in Context) and Visual Genome provide images with corresponding textual descriptions or annotations.\\n2.\\tTextual Data: Large corpora of text, such as Wikipedia articles, books, and web pages, are used to pretrain the language model (Gemma).\\n3.\\tAnnotated Multimodal Data: Datasets specifically designed for tasks like visual question answering (VQA), image captioning, and scene understanding, which combine both image and text annotations.\\nData for SigLIP\\nSigLIP\\'s training data includes:\\n1.\\tImage Datasets: High-quality and diverse image datasets like ImageNet, COCO, and Open Images.\\n2.\\tPaired Image-Text Data: Data where each image is paired with descriptive text, aiding the model in learning the relationships between visual content and language.\\nData for Gemma\\nGemma\\'s data requirements are primarily textual:\\n1.\\tLarge-Scale Text Corpora: Datasets such as the Common Crawl, Wikipedia, and BookCorpus provide extensive text data for pretraining.\\n2.\\tSpecialized Text Datasets: Fine-tuning datasets tailored to specific tasks like sentiment analysis, question answering (SQuAD), and natural language inference.\\nModel Building\\nBuilding Paligemma\\n1.\\tData Collection and Preprocessing:\\n○\\tCollect and clean large volumes of multimodal data.\\n○\\tPreprocess images (resizing, normalization) and text (tokenization, normalization).\\n2.\\tPretraining:\\n○\\tTrain SigLIP on image-text pairs to learn cross-modal representations.\\n○\\tTrain Gemma on large-scale text corpora to develop a deep understanding of language.\\n3.\\tMultimodal Fusion:\\n○\\tDevelop and train the fusion layer to integrate visual and textual features.\\n○\\tUtilize techniques like cross-attention to effectively combine modalities.\\n4.\\tFine-Tuning:\\n○\\tFine-tune the integrated model on task-specific multimodal datasets.\\n○\\tUse transfer learning to adapt pre-trained models to new tasks with limited data.\\nBuilding SigLIP\\n1.\\tImage Encoder Training:\\n○\\tUse convolutional neural networks (CNNs) or Vision Transformers (ViTs) to extract features from images.\\n○\\tTrain on large image datasets to develop robust visual representations.\\n2.\\tText Encoder Training:\\n○\\tEmploy transformers to process textual descriptions associated with images.\\n○\\tTrain on paired image-text data to learn the correlation between visual and textual information.\\n3.\\tCross-Modal Pretraining:\\n○\\tImplement cross-modal attention mechanisms to align image features with textual features.\\n○\\tPretrain on large-scale datasets to learn rich, shared representations.\\nBuilding Gemma\\n1.\\tEmbedding Layer Setup:\\n○\\tInitialize word embeddings using pre-trained vectors or train embeddings from scratch on a large text corpus.\\n2.\\tTransformer Training:\\n○\\tUse transformer architecture with multiple layers of self-attention and feed-forward networks.\\n○\\tPretrain on extensive text data to capture language patterns and contextual relationships.\\n3.\\tContextual Understanding:\\n○\\tIntegrate advanced attention mechanisms to focus on relevant text segments.\\n○\\tTrain on datasets like books, articles, and dialogues to understand various contexts and nuances.\\nModel Architecture\\nArchitecture of Paligemma\\n1.\\tInput Processing Module:\\n○\\tSigLIP Vision Processing: Image encoder + text encoder + cross-modal attention.\\n○\\tGemma Language Processing: Transformer-based language model.\\n2.\\tMultimodal Fusion Layer:\\n○\\tCross-attention mechanisms to combine image and text features.\\n○\\tDense layers and normalization to create a unified representation.\\n3.\\tCore Understanding Engine:\\n○\\tContextual understanding through integrated representations.\\n○\\tExternal knowledge integration via APIs or databases to enhance comprehension.\\n4.\\tOutput Generation Module:\\n○\\tDecoders for generating textual responses or actions.\\n○\\tTask-specific layers for applications like VQA or image captioning.\\n5.\\tFeedback Loop:\\n○\\tPerformance monitoring to identify strengths and weaknesses.\\n○\\tIterative learning to adapt and improve based on feedback.\\nArchitecture of SigLIP\\n1.\\tImage Encoder:\\n○\\tConvolutional layers or Vision Transformer layers to process images.\\n○\\tAttention mechanisms to focus on important visual features.\\n2.\\tText Encoder:\\n○\\tTransformer layers to process associated textual descriptions.\\n○\\tEmbedding layers to convert text into meaningful vectors.\\n3.\\tCross-Modal Attention Mechanism:\\n○\\tAttention layers connecting image and text encoders.\\n○\\tMechanisms to highlight relevant image regions based on text and vice versa.\\n4.\\tFusion Layer:\\n○\\tCombines image and text embeddings into a cohesive representation.\\n○\\tDense layers for integration and normalization.\\n5.\\tOutput Layer:\\n○\\tTask-specific layers for classification, captioning, or other vision tasks.\\n○\\tFine-tuning mechanisms to adapt to different applications.\\nArchitecture of Gemma\\n1.\\tEmbedding Layer:\\n○\\tConverts text into dense vector representations.\\n○\\tUtilizes pre-trained word embeddings or contextual embeddings.\\n2.\\tTransformer Layers:\\n○\\tMultiple layers of self-attention and feed-forward networks.\\n○\\tLayer normalization and residual connections for stable training.\\n3.\\tContextual Understanding Module:\\n○\\tAttention mechanisms to focus on relevant parts of the text.\\n○\\tMechanisms to maintain context over long text sequences.\\n4.\\tOutput Layer:\\n○\\tGenerates final text outputs, such as responses, summaries, or translations.\\n○\\tTask-specific adaptations for different language applications.\\nBy integrating these sophisticated components, Paligemma achieves a powerful synergy between vision and language, enabling it to perform complex tasks that require deep understanding and generation capabilities across both modalities.\\nFine-tuning Paligemma involves adjusting the pre-trained models (SigLIP for vision and Gemma for language) on a specific task using a tailored dataset. Here\\'s a detailed guide on how to fine-tune Paligemma, along with code examples.\\nPrerequisites\\n1.\\tData Preparation: You need a dataset relevant to your task, which contains both visual and textual information. For example, a Visual Question Answering (VQA) dataset.\\n2.\\tEnvironment Setup: Ensure you have a suitable machine learning environment with necessary libraries installed (e.g., PyTorch, Transformers, Vision libraries).\\nStep-by-Step Fine-Tuning Process\\n \\nPaliGemma is a vision-language model (VLM) developed by Google that can be fine-tuned for various tasks. Fine-tuning involves adjusting the model\\'s weights based on specific task requirements and datasets to improve its performance. Here\\'s a step-by-step guide on how to fine-tune PaliGemma:\\nStep 1: Download the Model and Dependencies\\n1.\\tDownload PaliGemma Model Checkpoint: Download the pre-trained PaliGemma model checkpoint and tokenizer from Kaggle or other sources.\\n2.\\tInstall Dependencies: Install the required dependencies, including JAX, TensorFlow, NumPy, and other libraries.\\nStep 2: Prepare the Model\\n1.\\tLoad the Model: Load the pre-trained PaliGemma model onto GPU devices.\\n2.\\tPrepare Inputs: Prepare the model\\'s inputs for training and inference by processing the data into the required format.\\nStep 3: Fine-tune the Model\\n1.\\tFreeze Parameters: Freeze the majority of the model\\'s parameters, except for the attention layers, to prevent overfitting.\\n2.\\tTrain the Model: Train the fine-tuned model using the specific task\\'s dataset and hyperparameters. This step can be done using JAX and other libraries.\\nStep 4: Test and Save the Model\\n1.\\tTest the Model: Test the fine-tuned model on a validation dataset to evaluate its performance.\\n2.\\tSave the Model: Save the fine-tuned model for later use by saving its weights and other parameters.\\nAdditional Tips\\n1.\\tUse a Small Version: For fine-tuning in Google Colab, use the smallest version of PaliGemma (paligemma-3b-pt-224) to limit GPU memory consumption.\\n2.\\tUse Roboflow Universe: Use Roboflow Universe for accessing and managing datasets for fine-tuning PaliGemma.\\n3.\\tFine-tune for Specific Tasks: Fine-tune PaliGemma for specific tasks such as object detection, segmentation, or text reading from images\\n\\n1. Import Necessary Libraries\\n \\n\\n2. Prepare the Dataset\\nDefine a custom dataset class to handle your data.\\n \\n\\n3. Initialize Pre-trained Models\\nLoad pre-trained models for vision and language.\\n \\n4. Define the Multimodal Model\\nCombine vision and language models into a single architecture.\\n \\n\\n5. Prepare for Training\\nSet up the data loaders, loss function, and optimizer.\\n \\n6. Fine-Tuning the Model\\nDefine the training loop to fine-tune the model.\\n \\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = loader.load()\n",
    "doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.CharacterTextSplitter at 0x269d3d315d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2365, which is longer than the specified 1000\n",
      "Created a chunk of size 2685, which is longer than the specified 1000\n",
      "Created a chunk of size 3380, which is longer than the specified 1000\n",
      "Created a chunk of size 3582, which is longer than the specified 1000\n",
      "Created a chunk of size 9716, which is longer than the specified 1000\n",
      "Created a chunk of size 2365, which is longer than the specified 1000\n",
      "Created a chunk of size 2685, which is longer than the specified 1000\n",
      "Created a chunk of size 3380, which is longer than the specified 1000\n",
      "Created a chunk of size 3582, which is longer than the specified 1000\n",
      "Created a chunk of size 9716, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83bf4b40e674d9092517eda8fb6c6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\.conda\\envs\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dhanu\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6a2f54cff1447abd167d538906e521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a34256421d2401fac6c3586f2e14ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8f2ff30bc04cb3a5a0e8e278f10bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc3c83a360e440dba2bba468ce7799a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5e0d2a1ca147198c9eba08a2f53173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c183409257244108f6b1fd748974efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24830e24db4942acba8cac0e914d905a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b971d36cb02414484fbe0810ff56c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e79c2b4e4e143f993aa62f6098460de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b06d22e855c4a0e8b5d61d77838c002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_function  = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\.conda\\envs\\llm\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "db = Chroma.from_documents(docs,embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x269fac32c20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain Paligemma'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Explain Paligemma'\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How Paligemma Works\\n1.\\tInput Reception: The system receives visual and textual inputs.\\n2.\\tProcessing: The inputs are processed through their respective modules—visual data through SigLIP and textual data through Gemma.\\n3.\\tIntegration: The multimodal fusion layer combines the processed data into a coherent representation.\\n4.\\tUnderstanding: The core understanding engine interprets the integrated data, using context and external knowledge.\\n5.\\tResponse Generation: An appropriate response is generated based on the interpretation.\\n6.\\tLearning and Adaptation: The system learns from interactions and feedback to improve its future performance.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = Chroma.from_documents(docs,embedding_function,persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x2698ea12ec0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "db3 = Chroma(embedding_function = embedding_function,persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x269a9fb4340>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = db3.similarity_search(query,k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How Paligemma Works\\n1.\\tInput Reception: The system receives visual and textual inputs.\\n2.\\tProcessing: The inputs are processed through their respective modules—visual data through SigLIP and textual data through Gemma.\\n3.\\tIntegration: The multimodal fusion layer combines the processed data into a coherent representation.\\n4.\\tUnderstanding: The core understanding engine interprets the integrated data, using context and external knowledge.\\n5.\\tResponse Generation: An appropriate response is generated based on the interpretation.\\n6.\\tLearning and Adaptation: The system learns from interactions and feedback to improve its future performance.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
