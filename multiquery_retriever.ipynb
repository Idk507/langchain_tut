{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiqueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from getpass import getpass\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = open(\"sample.txt\",\"r\",encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\.conda\\envs\\llm\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(repo_id= 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "                     model_kwargs={'temperature':0.5,'max_length':100}\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | MediumOpen in appSign upSign inWriteSign upSign inPPO AlgorithmDhanushKumar·Follow10 min read·Feb 21, 2024--ListenShareProximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John Schulman in 2017,had become the default reinforcement learning algorithm at American artificial intelligence company OpenAI.Many experts called PPO the state of the art because it seems to strike a balance between performance and comprehension. Compared with other algorithms, the three main advantages of PPO are simplicity, stability, and sample efficiency.PPO is classified as a policy gradient method for training an agent’s policy network. The policy network is the function that the agent uses to make decisions. Essentially, to train the right policy network, PPO takes a small policy update (step size), so the agent can reliably reach the optimal solution. A too-big step may direct policy in the false direction, thus having little possibility of recovery; a too-small step lowers overall efficiency. Consequently, PPO implements a clip function that constrains the policy update of an agent from being too large or too small.What is PPO?A policy, in Reinforcement Learning terminology, is a mapping from action space to state space. It can be imagined to be instructions for the RL agent, in terms of what actions it should take based upon which state of the environment it is currently in.When we talk about evaluating an agent, we generally mean evaluating the policy function to find out how well the agent is performing, following the given policy. This is where Policy Gradient methods play a vital role. When an agent is “learning” and doesn’t really know which actions yield the best result in the corresponding states, it does so by calculating the policy gradients. It works like a neural network architecture, whereby the gradient of the output, i.e, the log of probabilities of actions in that particular state, is taken with respect to parameters of the environment and the change is reflected in the policy, based upon the gradients.While this tried and tested method works well, the major disadvantages with these methods is their hypersensitivity to hyperparameter tuning such as choice of stepsize, learning rate, etc , along with their poor sample efficiency. Unlike supervised learning which has a guaranteed route to success or convergence with relatively less hyperparameter tuning, reinforcement learning is a lot more complex with various moving parts that need to be considered. PPO aims to strike a balance between important factors like ease of implementation, ease of tuning, sample complexity,sample efficiency and trying to compute an update at each step that minimizes the cost function while ensuring the deviation from the previous policy is relatively small. PPO is in fact, a policy gradient method that learns from online data as well. It merely ensures that the updated policy isn’t too much different from the old policy to ensure low variance in training.Here’s a brief overview of how PPO works:Policy Gradient Methods: PPO is based on policy gradient methods, which directly optimize the policy function that maps states to actions. This is in contrast to methods that estimate value functions.Objective Function: PPO aims to maximize the expected cumulative reward obtained from interacting with the environment. This is typically done by maximizing a surrogate objective function that approximates the policy improvement.Clipped Surrogate Objective: One of the key features of PPO is the clipped surrogate objective, which prevents large policy updates that could lead to catastrophic outcomes. By clipping the ratio between the probability of actions under the new policy and the old policy, PPO ensures that the policy update stays within a safe range.Multiple Epochs and Mini-Batch Updates: PPO typically involves multiple epochs of interaction with the environment, during which trajectories are collected. These trajectories are then used to compute the surrogate objective function, which is optimized using mini-batch updates.Value Function Estimation: PPO often incorporates value function estimation to reduce variance in the gradient estimates. This helps stabilize training and improve sample efficiency.Parallelization: PPO can be parallelized to accelerate training by collecting trajectories from multiple instances of the environment simultaneously.Architecture Of PPO:The architecture of PPO (Proximal Policy Optimization) primarily refers to the neural network architecture used to represent the policy and value functions in the context of deep reinforcement learning. Here’s a breakdown of the typical components:Policy Network: The policy network is a neural network that takes the current state of the environment as input and outputs a probability distribution over possible actions. This distribution represents the agent’s policy, which specifies the probabilities of taking each action given the current state. The architecture of the policy network can vary depending on the complexity of the environment and the task at hand. Common choices include feedforward neural networks, recurrent neural networks (RNNs), or convolutional neural networks (CNNs), depending on whether the state space is structured (e.g., images) or sequential (e.g., time-series data).Value Network: In addition to the policy network, PPO often includes a value network. The value network estimates the expected cumulative reward (value) of being in a given state. This estimation helps reduce the variance of the policy gradient estimates and provides additional signal for learning. The architecture of the value network can be similar to the policy network, although it typically outputs a single value instead of a probability distribution.Activation Functions: Within both the policy and value networks, various activation functions are used to introduce non-linearity into the network. Common choices include rectified linear units (ReLU), sigmoid, or hyperbolic tangent (tanh), depending on the requirements of the specific task and network architecture.Loss Functions: PPO uses loss functions to train both the policy and value networks. For the policy network, the loss function is typically based on the surrogate objective, which aims to maximize the expected cumulative reward subject to a constraint on the policy update size. For the value network, the loss function is often based on the mean squared error (MSE) between the predicted and actual values.Optimization Algorithm: PPO employs optimization algorithms to update the parameters of the policy and value networks based on the computed loss functions. Stochastic gradient descent (SGD) variants like Adam or RMSProp are commonly used for this purpose.Example: Navigating a Grid WorldIn this example, our agent is placed in a grid world environment where it needs to navigate from a starting point to a goal location while avoiding obstacles. The agent receives a positive reward when it reaches the goal and a negative reward if it collides with an obstacle. We’ll use PPO to train the agent to learn an optimal policy for navigating the grid world.Components:State Space: The grid world is represented as a discrete set of states, where each state corresponds to a location in the grid.Action Space: The agent can take discrete actions such as moving up, down, left, or right.Rewards: The agent receives a reward of +10 when reaching the goal, -10 when colliding with an obstacle, and -1 for each step taken.PPO Algorithm Steps:Initialize Policy Network: We start by initializing a neural network that represents the policy. This network takes the current state as input and outputs a probability distribution over possible actions.Collect Trajectories: The agent interacts with the environment by following its current policy. During this interaction, it collects trajectories consisting of states, actions, and rewards.Compute Advantage Estimates: Using the collected trajectories, we compute estimates of the advantages for each state-action pair. Advantage estimates represent how much better or worse an action is compared to the average action taken from a given state.Compute Surrogate Objective: We compute the surrogate objective, which is a function of the policy’s old and new parameters and the advantage estimates. This objective approximates policy improvement while ensuring that the policy update remains within a safe range.Optimize Policy Network: We use stochastic gradient descent (SGD) or a variant to update the parameters of the policy network, minimizing the surrogate objective.Repeat: Steps 2–5 are repeated for multiple iterations or until convergence.Example Iteration:Initialization: We initialize the policy network randomly.Interaction with Environment: The agent follows its current policy to navigate the grid world, collecting trajectories.Advantage Estimation: Using the collected trajectories, we estimate the advantages for each state-action pair based on the rewards received.Compute Surrogate Objective: We compute the surrogate objective using the advantage estimates and the policy’s old and new parameters.Optimize Policy Network: We update the parameters of the policy network using SGD to minimize the surrogate objective.Evaluation: We evaluate the updated policy by letting the agent navigate the grid world again and observe its performance.Repeat: Steps 2–6 are repeated for multiple iterations until the policy converges to an optimal solution.Through this iterative process, the agent gradually learns an optimal policy for navigating the grid world, balancing exploration and exploitation to maximize cumulative rewards while avoiding obstacles. PPO ensures stable and efficient learning by constraining the policy updates and leveraging advantages estimates to guide the learning process.import tensorflow as tfimport numpy as npimport gym# Environment setupenv = gym.make(\\'CartPole-v1\\')state_size = env.observation_space.shape[0]action_size = env.action_space.n# Hyperparametersgamma = 0.99  # Discount factorlr_actor = 0.001  # Actor learning ratelr_critic = 0.001  # Critic learning rateclip_ratio = 0.2  # PPO clip ratioepochs = 10  # Number of optimization epochsbatch_size = 64  # Batch size for optimization# Actor and Critic networksclass ActorCritic(tf.keras.Model):    def __init__(self, state_size, action_size):        super(ActorCritic, self).__init__()        self.dense1 = tf.keras.layers.Dense(64, activation=\\'relu\\')        self.policy_logits = tf.keras.layers.Dense(action_size)        self.dense2 = tf.keras.layers.Dense(64, activation=\\'relu\\')        self.value = tf.keras.layers.Dense(1)    def call(self, state):        x = self.dense1(state)        logits = self.policy_logits(x)        value = self.value(x)        return logits, value# PPO algorithmdef ppo_loss(old_logits, old_values, advantages, states, actions, returns):    def compute_loss(logits, values, actions, returns):        actions_onehot = tf.one_hot(actions, action_size, dtype=tf.float32)        policy = tf.nn.softmax(logits)        action_probs = tf.reduce_sum(actions_onehot * policy, axis=1)        old_policy = tf.nn.softmax(old_logits)        old_action_probs = tf.reduce_sum(actions_onehot * old_policy, axis=1)        # Policy loss        ratio = tf.exp(tf.math.log(action_probs + 1e-10) - tf.math.log(old_action_probs + 1e-10))        clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)        policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))        # Value loss        value_loss = tf.reduce_mean(tf.square(values - returns))        # Entropy bonus (optional)        entropy_bonus = tf.reduce_mean(policy * tf.math.log(policy + 1e-10))        total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_bonus  # Entropy regularization        return total_loss    def get_advantages(returns, values):        advantages = returns - values        return (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-8)    def train_step(states, actions, returns, old_logits, old_values):        with tf.GradientTape() as tape:            logits, values = model(states)            loss = compute_loss(logits, values, actions, returns)        gradients = tape.gradient(loss, model.trainable_variables)        optimizer.apply_gradients(zip(gradients, model.trainable_variables))        return loss    advantages = get_advantages(returns, old_values)    for _ in range(epochs):        loss = train_step(states, actions, returns, old_logits, old_values)    return loss# Initialize actor-critic model and optimizermodel = ActorCritic(state_size, action_size)optimizer = tf.keras.optimizers.Adam(learning_rate=lr_actor)# Main training loopmax_episodes = 1000max_steps_per_episode = 1000for episode in range(max_episodes):    states, actions, rewards, values, returns = [], [], [], [], []    state = env.reset()    for step in range(max_steps_per_episode):        state = tf.expand_dims(tf.convert_to_tensor(state), 0)        logits, value = model(state)        # Sample action from the policy distribution        action = tf.random.categorical(logits, 1)[0, 0].numpy()        next_state, reward, done, _ = env.step(action)        states.append(state)        actions.append(action)        rewards.append(reward)        values.append(value)        state = next_state        if done:            returns_batch = []            discounted_sum = 0            for r in rewards[::-1]:                discounted_sum = r + gamma * discounted_sum                returns_batch.append(discounted_sum)            returns_batch.reverse()            states = tf.concat(states, axis=0)            actions = np.array(actions, dtype=np.int32)            values = tf.concat(values, axis=0)            returns_batch = tf.convert_to_tensor(returns_batch)            old_logits, _ = model(states)            loss = ppo_loss(old_logits, values, returns_batch - np.array(values), states, actions, returns_batch)            print(f\"Episode: {episode + 1}, Loss: {loss.numpy()}\")            breakThis code implements the PPO algorithm with a simple actor-critic neural network architecture using TensorFlow. The actor network outputs the logits of the policy distribution, while the critic network estimates the state value. The loss function combines the policy gradient loss and value loss with a PPO-style clipped objective. The training loop collects trajectories from the environment, computes advantages, and performs PPO updates for a specified number of epochs. Finally, the code trains the agent on the CartPole environment from OpenAI Gym for a specified number of episodes.Advantage:The advantage A(s,a) measures how good or bad it is to take a specific action a in a particular state s compared to the average action value in that state.Mathematically, the advantage is calculated as the difference between the observed return Gt\\u200b (the sum of rewards from time step t onward) and the state value A(s,a)=Gt\\u200b−V(s)The advantage represents how much better or worse an action is compared to the average expected return from that state. Positive values indicate that the action was better than average, while negative values indicate the action was worse than average.Policy Gradient:The policy gradient ∇θ\\u200bJ(θ) represents the gradient of the expected return with respect to the policy parameters θ.Mathematically, it’s calculated using the policy gradient theorem: ∇θ\\u200bJ(θ)=Eπ\\u200b[∇θ\\u200blogπ(a∣s)⋅A(s,a)]This equation tells us that we should update the policy parameters in the direction that increases the log probability of actions that yielded high advantages.Surrogate Objective:In PPO, we maximize a surrogate objective function L(θ), which approximates the policy improvement.The surrogate objective is designed to maximize the expected return while ensuring that the policy update is not too large, which could lead to instability.Mathematically, the surrogate objective is a combination of the policy loss Lpolicy(θ) and the value loss Lvalue(θ), along with an optional entropy bonus: L(θ)=Lpolicy(θ)+αLvalue(θ)−βH(π(⋅∣s))Here, α and β are hyperparameters controlling the relative importance of the value loss and entropy bonus, respectively.Clipped Surrogate Objective:One of the key features of PPO is the clipped surrogate objective, which prevents large policy updates that could lead to catastrophic outcomes.To prevent large updates, the ratio between the probability of actions under the new policy and the old policy is clipped to a specified range.Mathematically, the clipped surrogate objective is defined as:is the ratio of the probability of taking action at\\u200b under the new policy to the old policy, and ϵ is the clipping parameter.Ratio FunctionIn PPO, the ratio function calculates the probability of taking action a at state s in the current policy network divided by the previous old version of policy.In this function, rt(θ) denotes the probability ratio between the current and old policy:If rt(θ)>1, the action a at state s is more likely based on the current policy than the old policy.If rt(θ) is between 0 and 1, the action a at state s is less likely based on the current policy than the old policy.This ratio function can easily estimate the divergence between old and current policiesPPO’s Objective FunctionR-theta * Advantage Function: this is the product of the ratio function and the advantage function that was introduced in TRPO, also known as normal policy gradient objectiveClipped (R-theta) * Advantage Function:The policy ratio is first clipped between 1- epsilon and 1 + epsilon; generally, epsilon is defined to be 0.20. Then, multiply the clipped version by the advantage.PpoReinforcement LearningTensorFlowReinforcementPolicy----FollowWritten by DhanushKumar104 FollowersData Science || Machine Learning ||Deep Learning|| Language Models || GenAI contact: danushidk507@gmail.comFollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader('https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a')\n",
    "\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.RecursiveCharacterTextSplitter at 0x1a59b12bee0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=0)\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | MediumOpen in appSign upSign inWriteSign upSign inPPO AlgorithmDhanushKumar·Follow10 min read·Feb 21, 2024--ListenShareProximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John Schulman in 2017,had become the default reinforcement learning algorithm at American artificial intelligence', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='company OpenAI.Many experts called PPO the state of the art because it seems to strike a balance between performance and comprehension. Compared with other algorithms, the three main advantages of PPO are simplicity, stability, and sample efficiency.PPO is classified as a policy gradient method for training an agent’s policy network. The policy network is the function that the agent uses to make decisions. Essentially, to train the right policy network, PPO takes a small policy update (step', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='size), so the agent can reliably reach the optimal solution. A too-big step may direct policy in the false direction, thus having little possibility of recovery; a too-small step lowers overall efficiency. Consequently, PPO implements a clip function that constrains the policy update of an agent from being too large or too small.What is PPO?A policy, in Reinforcement Learning terminology, is a mapping from action space to state space. It can be imagined to be instructions for the RL agent, in', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='terms of what actions it should take based upon which state of the environment it is currently in.When we talk about evaluating an agent, we generally mean evaluating the policy function to find out how well the agent is performing, following the given policy. This is where Policy Gradient methods play a vital role. When an agent is “learning” and doesn’t really know which actions yield the best result in the corresponding states, it does so by calculating the policy gradients. It works like a', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='neural network architecture, whereby the gradient of the output, i.e, the log of probabilities of actions in that particular state, is taken with respect to parameters of the environment and the change is reflected in the policy, based upon the gradients.While this tried and tested method works well, the major disadvantages with these methods is their hypersensitivity to hyperparameter tuning such as choice of stepsize, learning rate, etc , along with their poor sample efficiency. Unlike', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='supervised learning which has a guaranteed route to success or convergence with relatively less hyperparameter tuning, reinforcement learning is a lot more complex with various moving parts that need to be considered. PPO aims to strike a balance between important factors like ease of implementation, ease of tuning, sample complexity,sample efficiency and trying to compute an update at each step that minimizes the cost function while ensuring the deviation from the previous policy is relatively', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='small. PPO is in fact, a policy gradient method that learns from online data as well. It merely ensures that the updated policy isn’t too much different from the old policy to ensure low variance in training.Here’s a brief overview of how PPO works:Policy Gradient Methods: PPO is based on policy gradient methods, which directly optimize the policy function that maps states to actions. This is in contrast to methods that estimate value functions.Objective Function: PPO aims to maximize the', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='expected cumulative reward obtained from interacting with the environment. This is typically done by maximizing a surrogate objective function that approximates the policy improvement.Clipped Surrogate Objective: One of the key features of PPO is the clipped surrogate objective, which prevents large policy updates that could lead to catastrophic outcomes. By clipping the ratio between the probability of actions under the new policy and the old policy, PPO ensures that the policy update stays', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='within a safe range.Multiple Epochs and Mini-Batch Updates: PPO typically involves multiple epochs of interaction with the environment, during which trajectories are collected. These trajectories are then used to compute the surrogate objective function, which is optimized using mini-batch updates.Value Function Estimation: PPO often incorporates value function estimation to reduce variance in the gradient estimates. This helps stabilize training and improve sample efficiency.Parallelization:', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='PPO can be parallelized to accelerate training by collecting trajectories from multiple instances of the environment simultaneously.Architecture Of PPO:The architecture of PPO (Proximal Policy Optimization) primarily refers to the neural network architecture used to represent the policy and value functions in the context of deep reinforcement learning. Here’s a breakdown of the typical components:Policy Network: The policy network is a neural network that takes the current state of the', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='environment as input and outputs a probability distribution over possible actions. This distribution represents the agent’s policy, which specifies the probabilities of taking each action given the current state. The architecture of the policy network can vary depending on the complexity of the environment and the task at hand. Common choices include feedforward neural networks, recurrent neural networks (RNNs), or convolutional neural networks (CNNs), depending on whether the state space is', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='structured (e.g., images) or sequential (e.g., time-series data).Value Network: In addition to the policy network, PPO often includes a value network. The value network estimates the expected cumulative reward (value) of being in a given state. This estimation helps reduce the variance of the policy gradient estimates and provides additional signal for learning. The architecture of the value network can be similar to the policy network, although it typically outputs a single value instead of a', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='probability distribution.Activation Functions: Within both the policy and value networks, various activation functions are used to introduce non-linearity into the network. Common choices include rectified linear units (ReLU), sigmoid, or hyperbolic tangent (tanh), depending on the requirements of the specific task and network architecture.Loss Functions: PPO uses loss functions to train both the policy and value networks. For the policy network, the loss function is typically based on the', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='surrogate objective, which aims to maximize the expected cumulative reward subject to a constraint on the policy update size. For the value network, the loss function is often based on the mean squared error (MSE) between the predicted and actual values.Optimization Algorithm: PPO employs optimization algorithms to update the parameters of the policy and value networks based on the computed loss functions. Stochastic gradient descent (SGD) variants like Adam or RMSProp are commonly used for', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='this purpose.Example: Navigating a Grid WorldIn this example, our agent is placed in a grid world environment where it needs to navigate from a starting point to a goal location while avoiding obstacles. The agent receives a positive reward when it reaches the goal and a negative reward if it collides with an obstacle. We’ll use PPO to train the agent to learn an optimal policy for navigating the grid world.Components:State Space: The grid world is represented as a discrete set of states, where', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='each state corresponds to a location in the grid.Action Space: The agent can take discrete actions such as moving up, down, left, or right.Rewards: The agent receives a reward of +10 when reaching the goal, -10 when colliding with an obstacle, and -1 for each step taken.PPO Algorithm Steps:Initialize Policy Network: We start by initializing a neural network that represents the policy. This network takes the current state as input and outputs a probability distribution over possible', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='actions.Collect Trajectories: The agent interacts with the environment by following its current policy. During this interaction, it collects trajectories consisting of states, actions, and rewards.Compute Advantage Estimates: Using the collected trajectories, we compute estimates of the advantages for each state-action pair. Advantage estimates represent how much better or worse an action is compared to the average action taken from a given state.Compute Surrogate Objective: We compute the', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='surrogate objective, which is a function of the policy’s old and new parameters and the advantage estimates. This objective approximates policy improvement while ensuring that the policy update remains within a safe range.Optimize Policy Network: We use stochastic gradient descent (SGD) or a variant to update the parameters of the policy network, minimizing the surrogate objective.Repeat: Steps 2–5 are repeated for multiple iterations or until convergence.Example Iteration:Initialization: We', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='initialize the policy network randomly.Interaction with Environment: The agent follows its current policy to navigate the grid world, collecting trajectories.Advantage Estimation: Using the collected trajectories, we estimate the advantages for each state-action pair based on the rewards received.Compute Surrogate Objective: We compute the surrogate objective using the advantage estimates and the policy’s old and new parameters.Optimize Policy Network: We update the parameters of the policy', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='network using SGD to minimize the surrogate objective.Evaluation: We evaluate the updated policy by letting the agent navigate the grid world again and observe its performance.Repeat: Steps 2–6 are repeated for multiple iterations until the policy converges to an optimal solution.Through this iterative process, the agent gradually learns an optimal policy for navigating the grid world, balancing exploration and exploitation to maximize cumulative rewards while avoiding obstacles. PPO ensures', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content=\"stable and efficient learning by constraining the policy updates and leveraging advantages estimates to guide the learning process.import tensorflow as tfimport numpy as npimport gym# Environment setupenv = gym.make('CartPole-v1')state_size = env.observation_space.shape[0]action_size = env.action_space.n# Hyperparametersgamma = 0.99  # Discount factorlr_actor = 0.001  # Actor learning ratelr_critic = 0.001  # Critic learning rateclip_ratio = 0.2  # PPO clip ratioepochs = 10  # Number of\", metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content=\"optimization epochsbatch_size = 64  # Batch size for optimization# Actor and Critic networksclass ActorCritic(tf.keras.Model):    def __init__(self, state_size, action_size):        super(ActorCritic, self).__init__()        self.dense1 = tf.keras.layers.Dense(64, activation='relu')        self.policy_logits = tf.keras.layers.Dense(action_size)        self.dense2 = tf.keras.layers.Dense(64, activation='relu')        self.value = tf.keras.layers.Dense(1)    def call(self, state):        x =\", metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='self.dense1(state)        logits = self.policy_logits(x)        value = self.value(x)        return logits, value# PPO algorithmdef ppo_loss(old_logits, old_values, advantages, states, actions, returns):    def compute_loss(logits, values, actions, returns):        actions_onehot = tf.one_hot(actions, action_size, dtype=tf.float32)        policy = tf.nn.softmax(logits)        action_probs = tf.reduce_sum(actions_onehot * policy, axis=1)        old_policy = tf.nn.softmax(old_logits)', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='old_action_probs = tf.reduce_sum(actions_onehot * old_policy, axis=1)        # Policy loss        ratio = tf.exp(tf.math.log(action_probs + 1e-10) - tf.math.log(old_action_probs + 1e-10))        clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)        policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))        # Value loss        value_loss = tf.reduce_mean(tf.square(values - returns))        # Entropy bonus (optional)', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='entropy_bonus = tf.reduce_mean(policy * tf.math.log(policy + 1e-10))        total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_bonus  # Entropy regularization        return total_loss    def get_advantages(returns, values):        advantages = returns - values        return (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-8)    def train_step(states, actions, returns, old_logits, old_values):        with tf.GradientTape() as tape:            logits,', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='values = model(states)            loss = compute_loss(logits, values, actions, returns)        gradients = tape.gradient(loss, model.trainable_variables)        optimizer.apply_gradients(zip(gradients, model.trainable_variables))        return loss    advantages = get_advantages(returns, old_values)    for _ in range(epochs):        loss = train_step(states, actions, returns, old_logits, old_values)    return loss# Initialize actor-critic model and optimizermodel = ActorCritic(state_size,', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='action_size)optimizer = tf.keras.optimizers.Adam(learning_rate=lr_actor)# Main training loopmax_episodes = 1000max_steps_per_episode = 1000for episode in range(max_episodes):    states, actions, rewards, values, returns = [], [], [], [], []    state = env.reset()    for step in range(max_steps_per_episode):        state = tf.expand_dims(tf.convert_to_tensor(state), 0)        logits, value = model(state)        # Sample action from the policy distribution        action =', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='tf.random.categorical(logits, 1)[0, 0].numpy()        next_state, reward, done, _ = env.step(action)        states.append(state)        actions.append(action)        rewards.append(reward)        values.append(value)        state = next_state        if done:            returns_batch = []            discounted_sum = 0            for r in rewards[::-1]:                discounted_sum = r + gamma * discounted_sum                returns_batch.append(discounted_sum)            returns_batch.reverse()', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='states = tf.concat(states, axis=0)            actions = np.array(actions, dtype=np.int32)            values = tf.concat(values, axis=0)            returns_batch = tf.convert_to_tensor(returns_batch)            old_logits, _ = model(states)            loss = ppo_loss(old_logits, values, returns_batch - np.array(values), states, actions, returns_batch)            print(f\"Episode: {episode + 1}, Loss: {loss.numpy()}\")            breakThis code implements the PPO algorithm with a simple', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='actor-critic neural network architecture using TensorFlow. The actor network outputs the logits of the policy distribution, while the critic network estimates the state value. The loss function combines the policy gradient loss and value loss with a PPO-style clipped objective. The training loop collects trajectories from the environment, computes advantages, and performs PPO updates for a specified number of epochs. Finally, the code trains the agent on the CartPole environment from OpenAI Gym', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='for a specified number of episodes.Advantage:The advantage A(s,a) measures how good or bad it is to take a specific action a in a particular state s compared to the average action value in that state.Mathematically, the advantage is calculated as the difference between the observed return Gt\\u200b (the sum of rewards from time step t onward) and the state value A(s,a)=Gt\\u200b−V(s)The advantage represents how much better or worse an action is compared to the average expected return from that state.', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='Positive values indicate that the action was better than average, while negative values indicate the action was worse than average.Policy Gradient:The policy gradient ∇θ\\u200bJ(θ) represents the gradient of the expected return with respect to the policy parameters θ.Mathematically, it’s calculated using the policy gradient theorem: ∇θ\\u200bJ(θ)=Eπ\\u200b[∇θ\\u200blogπ(a∣s)⋅A(s,a)]This equation tells us that we should update the policy parameters in the direction that increases the log probability of actions that', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='yielded high advantages.Surrogate Objective:In PPO, we maximize a surrogate objective function L(θ), which approximates the policy improvement.The surrogate objective is designed to maximize the expected return while ensuring that the policy update is not too large, which could lead to instability.Mathematically, the surrogate objective is a combination of the policy loss Lpolicy(θ) and the value loss Lvalue(θ), along with an optional entropy bonus: L(θ)=Lpolicy(θ)+αLvalue(θ)−βH(π(⋅∣s))Here, α', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='and β are hyperparameters controlling the relative importance of the value loss and entropy bonus, respectively.Clipped Surrogate Objective:One of the key features of PPO is the clipped surrogate objective, which prevents large policy updates that could lead to catastrophic outcomes.To prevent large updates, the ratio between the probability of actions under the new policy and the old policy is clipped to a specified range.Mathematically, the clipped surrogate objective is defined as:is the', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='ratio of the probability of taking action at\\u200b under the new policy to the old policy, and ϵ is the clipping parameter.Ratio FunctionIn PPO, the ratio function calculates the probability of taking action a at state s in the current policy network divided by the previous old version of policy.In this function, rt(θ) denotes the probability ratio between the current and old policy:If rt(θ)>1, the action a at state s is more likely based on the current policy than the old policy.If rt(θ) is between', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='0 and 1, the action a at state s is less likely based on the current policy than the old policy.This ratio function can easily estimate the divergence between old and current policiesPPO’s Objective FunctionR-theta * Advantage Function: this is the product of the ratio function and the advantage function that was introduced in TRPO, also known as normal policy gradient objectiveClipped (R-theta) * Advantage Function:The policy ratio is first clipped between 1- epsilon and 1 + epsilon;', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'}),\n",
       " Document(page_content='generally, epsilon is defined to be 0.20. Then, multiply the clipped version by the advantage.PpoReinforcement LearningTensorFlowReinforcementPolicy----FollowWritten by DhanushKumar104 FollowersData Science || Machine Learning ||Deep Learning|| Language Models || GenAI contact: danushidk507@gmail.comFollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams', metadata={'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium', 'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = text_splitter.split_documents(data)\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-mpnet-base-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=split,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Explain PPO Algorithm\"\n",
    "\n",
    "reteriver_from_llm = MultiQueryRetriever.from_llm(retriever=vectordb.as_retriever(),llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001A5C05AA0E0>) llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}'), llm=HuggingFaceHub(client=<InferenceClient(model='mistralai/Mixtral-8x7B-Instruct-v0.1', timeout=None)>, repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1', task='text-generation', model_kwargs={'temperature': 0.5, 'max_length': 100}), output_parser=LineListOutputParser())\n"
     ]
    }
   ],
   "source": [
    "print(reteriver_from_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "\n",
    "logging.getLogger(\"langchain.retriever.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\.conda\\envs\\llm\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "unique_docs = reteriver_from_llm.get_relevant_documents(query=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='environment as input and outputs a probability distribution over possible actions. This distribution represents the agent’s policy, which specifies the probabilities of taking each action given the current state. The architecture of the policy network can vary depending on the complexity of the environment and the task at hand. Common choices include feedforward neural networks, recurrent neural networks (RNNs), or convolutional neural networks (CNNs), depending on whether the state space is', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='generally, epsilon is defined to be 0.20. Then, multiply the clipped version by the advantage.PpoReinforcement LearningTensorFlowReinforcementPolicy----FollowWritten by DhanushKumar104 FollowersData Science || Machine Learning ||Deep Learning|| Language Models || GenAI contact: danushidk507@gmail.comFollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content=\"optimization epochsbatch_size = 64  # Batch size for optimization# Actor and Critic networksclass ActorCritic(tf.keras.Model):    def __init__(self, state_size, action_size):        super(ActorCritic, self).__init__()        self.dense1 = tf.keras.layers.Dense(64, activation='relu')        self.policy_logits = tf.keras.layers.Dense(action_size)        self.dense2 = tf.keras.layers.Dense(64, activation='relu')        self.value = tf.keras.layers.Dense(1)    def call(self, state):        x =\", metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='surrogate objective, which is a function of the policy’s old and new parameters and the advantage estimates. This objective approximates policy improvement while ensuring that the policy update remains within a safe range.Optimize Policy Network: We use stochastic gradient descent (SGD) or a variant to update the parameters of the policy network, minimizing the surrogate objective.Repeat: Steps 2–5 are repeated for multiple iterations or until convergence.Example Iteration:Initialization: We', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='tf.random.categorical(logits, 1)[0, 0].numpy()        next_state, reward, done, _ = env.step(action)        states.append(state)        actions.append(action)        rewards.append(reward)        values.append(value)        state = next_state        if done:            returns_batch = []            discounted_sum = 0            for r in rewards[::-1]:                discounted_sum = r + gamma * discounted_sum                returns_batch.append(discounted_sum)            returns_batch.reverse()', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='old_action_probs = tf.reduce_sum(actions_onehot * old_policy, axis=1)        # Policy loss        ratio = tf.exp(tf.math.log(action_probs + 1e-10) - tf.math.log(old_action_probs + 1e-10))        clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)        policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))        # Value loss        value_loss = tf.reduce_mean(tf.square(values - returns))        # Entropy bonus (optional)', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='ratio of the probability of taking action at\\u200b under the new policy to the old policy, and ϵ is the clipping parameter.Ratio FunctionIn PPO, the ratio function calculates the probability of taking action a at state s in the current policy network divided by the previous old version of policy.In this function, rt(θ) denotes the probability ratio between the current and old policy:If rt(θ)>1, the action a at state s is more likely based on the current policy than the old policy.If rt(θ) is between', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='surrogate objective, which aims to maximize the expected cumulative reward subject to a constraint on the policy update size. For the value network, the loss function is often based on the mean squared error (MSE) between the predicted and actual values.Optimization Algorithm: PPO employs optimization algorithms to update the parameters of the policy and value networks based on the computed loss functions. Stochastic gradient descent (SGD) variants like Adam or RMSProp are commonly used for', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='actions.Collect Trajectories: The agent interacts with the environment by following its current policy. During this interaction, it collects trajectories consisting of states, actions, and rewards.Compute Advantage Estimates: Using the collected trajectories, we compute estimates of the advantages for each state-action pair. Advantage estimates represent how much better or worse an action is compared to the average action taken from a given state.Compute Surrogate Objective: We compute the', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='for a specified number of episodes.Advantage:The advantage A(s,a) measures how good or bad it is to take a specific action a in a particular state s compared to the average action value in that state.Mathematically, the advantage is calculated as the difference between the observed return Gt\\u200b (the sum of rewards from time step t onward) and the state value A(s,a)=Gt\\u200b−V(s)The advantage represents how much better or worse an action is compared to the average expected return from that state.', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='initialize the policy network randomly.Interaction with Environment: The agent follows its current policy to navigate the grid world, collecting trajectories.Advantage Estimation: Using the collected trajectories, we estimate the advantages for each state-action pair based on the rewards received.Compute Surrogate Objective: We compute the surrogate objective using the advantage estimates and the policy’s old and new parameters.Optimize Policy Network: We update the parameters of the policy', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='this purpose.Example: Navigating a Grid WorldIn this example, our agent is placed in a grid world environment where it needs to navigate from a starting point to a goal location while avoiding obstacles. The agent receives a positive reward when it reaches the goal and a negative reward if it collides with an obstacle. We’ll use PPO to train the agent to learn an optimal policy for navigating the grid world.Components:State Space: The grid world is represented as a discrete set of states, where', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='neural network architecture, whereby the gradient of the output, i.e, the log of probabilities of actions in that particular state, is taken with respect to parameters of the environment and the change is reflected in the policy, based upon the gradients.While this tried and tested method works well, the major disadvantages with these methods is their hypersensitivity to hyperparameter tuning such as choice of stepsize, learning rate, etc , along with their poor sample efficiency. Unlike', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='terms of what actions it should take based upon which state of the environment it is currently in.When we talk about evaluating an agent, we generally mean evaluating the policy function to find out how well the agent is performing, following the given policy. This is where Policy Gradient methods play a vital role. When an agent is “learning” and doesn’t really know which actions yield the best result in the corresponding states, it does so by calculating the policy gradients. It works like a', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='company OpenAI.Many experts called PPO the state of the art because it seems to strike a balance between performance and comprehension. Compared with other algorithms, the three main advantages of PPO are simplicity, stability, and sample efficiency.PPO is classified as a policy gradient method for training an agent’s policy network. The policy network is the function that the agent uses to make decisions. Essentially, to train the right policy network, PPO takes a small policy update (step', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='size), so the agent can reliably reach the optimal solution. A too-big step may direct policy in the false direction, thus having little possibility of recovery; a too-small step lowers overall efficiency. Consequently, PPO implements a clip function that constrains the policy update of an agent from being too large or too small.What is PPO?A policy, in Reinforcement Learning terminology, is a mapping from action space to state space. It can be imagined to be instructions for the RL agent, in', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='expected cumulative reward obtained from interacting with the environment. This is typically done by maximizing a surrogate objective function that approximates the policy improvement.Clipped Surrogate Objective: One of the key features of PPO is the clipped surrogate objective, which prevents large policy updates that could lead to catastrophic outcomes. By clipping the ratio between the probability of actions under the new policy and the old policy, PPO ensures that the policy update stays', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | MediumOpen in appSign upSign inWriteSign upSign inPPO AlgorithmDhanushKumar·Follow10 min read·Feb 21, 2024--ListenShareProximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John Schulman in 2017,had become the default reinforcement learning algorithm at American artificial intelligence', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='small. PPO is in fact, a policy gradient method that learns from online data as well. It merely ensures that the updated policy isn’t too much different from the old policy to ensure low variance in training.Here’s a brief overview of how PPO works:Policy Gradient Methods: PPO is based on policy gradient methods, which directly optimize the policy function that maps states to actions. This is in contrast to methods that estimate value functions.Objective Function: PPO aims to maximize the', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='within a safe range.Multiple Epochs and Mini-Batch Updates: PPO typically involves multiple epochs of interaction with the environment, during which trajectories are collected. These trajectories are then used to compute the surrogate objective function, which is optimized using mini-batch updates.Value Function Estimation: PPO often incorporates value function estimation to reduce variance in the gradient estimates. This helps stabilize training and improve sample efficiency.Parallelization:', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='action_size)optimizer = tf.keras.optimizers.Adam(learning_rate=lr_actor)# Main training loopmax_episodes = 1000max_steps_per_episode = 1000for episode in range(max_episodes):    states, actions, rewards, values, returns = [], [], [], [], []    state = env.reset()    for step in range(max_steps_per_episode):        state = tf.expand_dims(tf.convert_to_tensor(state), 0)        logits, value = model(state)        # Sample action from the policy distribution        action =', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content=\"stable and efficient learning by constraining the policy updates and leveraging advantages estimates to guide the learning process.import tensorflow as tfimport numpy as npimport gym# Environment setupenv = gym.make('CartPole-v1')state_size = env.observation_space.shape[0]action_size = env.action_space.n# Hyperparametersgamma = 0.99  # Discount factorlr_actor = 0.001  # Actor learning ratelr_critic = 0.001  # Critic learning rateclip_ratio = 0.2  # PPO clip ratioepochs = 10  # Number of\", metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='yielded high advantages.Surrogate Objective:In PPO, we maximize a surrogate objective function L(θ), which approximates the policy improvement.The surrogate objective is designed to maximize the expected return while ensuring that the policy update is not too large, which could lead to instability.Mathematically, the surrogate objective is a combination of the policy loss Lpolicy(θ) and the value loss Lvalue(θ), along with an optional entropy bonus: L(θ)=Lpolicy(θ)+αLvalue(θ)−βH(π(⋅∣s))Here, α', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'}),\n",
       " Document(page_content='PPO can be parallelized to accelerate training by collecting trajectories from multiple instances of the environment simultaneously.Architecture Of PPO:The architecture of PPO (Proximal Policy Optimization) primarily refers to the neural network architecture used to represent the policy and value functions in the context of deep reinforcement learning. Here’s a breakdown of the typical components:Policy Network: The policy network is a neural network that takes the current state of the', metadata={'description': 'Proximal Policy Optimization (PPO) is an algorithm in the field of reinforcement learning that trains a computer agent’s decision function to accomplish difficult tasks. PPO was developed by John…', 'language': 'en', 'source': 'https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a', 'title': 'PPO Algorithm. Proximal Policy Optimization (PPO) is… | by DhanushKumar | Medium'})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
