{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('Finetuning PaliGemma.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x1ff7c76c5e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Paligemma\\n:\\nPaligemma\\nis\\na\\nsophisticated,\\nintegrated\\nAI\\nsystem\\nthat\\ncombines\\nvision\\nand\\nlanguage\\nmodels\\nto\\nprovide\\ncomprehensive\\nmultimodal\\nunderstanding\\nand\\ngeneration\\ncapabilities.\\nThe\\nname\\n\"Paligemma\"\\nsuggests\\na\\ncombination\\nof\\n\"Pali,\"\\npotentially\\nhinting\\nat\\na\\nfoundational\\nor\\nstructural\\naspect,\\nand\\n\"gemma,\"\\nwhich\\ncan\\nimply\\nsomething\\nvaluable\\nor\\nprecious,\\nindicating\\nthe\\nintegration\\nof\\ncrucial\\nAI\\ncomponents.\\nPaliGemma\\nis\\na\\nvision-language\\nmodel\\n(VLM)\\ndeveloped\\nby\\nGoogle.\\nIt\\nis\\na\\nmultimodal\\nmodel\\nthat\\ncombines\\nthe\\ncapabilities\\nof\\na\\nvision\\nmodel\\nand\\na\\nlanguage\\nmodel.\\nThe\\nmodel\\nis\\ncomposed\\nof\\na\\nSiglip-400m\\nvision\\nencoder\\nand\\na\\nGemma-2B\\ndecoder\\nlinked\\nby\\na\\nmultimodal\\nlinear\\nprojection.\\nPaliGemma\\nis\\ndesigned\\nto\\nprocess\\nboth\\nimages\\nand\\ntext\\nand\\ngenerate\\ntext\\nas\\noutput,\\nsupporting\\nmultiple\\nlanguages\\nKey\\nFeatur es\\nand\\nCapabilities\\n1.\\nMultimodal\\nComprehension:\\nPaliGemma\\ncan\\nsimultaneously\\nunderstand\\nboth\\nimages\\nand\\ntext,\\nmaking\\nit\\nsuitable\\nfor\\ntasks\\nsuch\\nas\\nimage\\ncaptioning,\\nvisual\\nquestion\\nanswering,\\nand\\ntext\\nreading\\nfrom\\nimages.\\n2.\\nFine-T uning:\\nPaliGemma\\nis\\ndesigned\\nto\\nbe\\nfine-tuned\\non\\nspecific\\ntasks,\\nwhich\\nallows\\nit\\nto\\nadapt\\nto\\ndif ferent\\nuse\\ncases\\nand\\nachieve\\nbetter\\nperformance.\\nThis\\nfine-tuning\\nprocess\\ninvolves\\nadjusting\\nthe\\nmodel\\'s\\nweights\\nbased\\non\\nthe\\nspecific\\ntask\\nand\\ndataset.\\n3.\\nPre-T raining:\\nPaliGemma\\nis\\npre-trained\\non\\na\\nvariety\\nof\\ndatasets,\\nincluding\\nW ebLI,\\nCC3M-35L,\\nVQ²A-CC3M-35L/VQG-CC3M-35L,\\nOpenImages,\\nand\\nWIT .\\nThis\\npre-training\\nhelps\\nthe\\nmodel\\nlearn\\ngeneral\\nrepresentations\\nof\\nimages\\nand\\ntext\\nthat\\ncan\\nbe\\nleveraged\\nfor\\ndownstream\\ntasks.', metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 0}),\n",
       " Document(page_content=\"4.\\nResolutions\\nand\\nPrecisions:\\nPaliGemma\\nmodels\\ncome\\nin\\nthree\\nresolutions\\n(224x224,\\n448x448,\\nand\\n896x896)\\nand\\nthree\\nprecisions\\n(bfloat16,\\nfloat16,\\nand\\nfloat32).\\nThe\\nhigher\\nresolutions\\nare\\nmore\\nmemory-intensive\\nbut\\ncan\\nbe\\nbeneficial\\nfor\\nfine-grained\\ntasks\\nlike\\noptical\\ncharacter\\nrecognition\\n(OCR).\\n5.\\nIntegration\\nwith\\nT ransformers:\\nPaliGemma\\nmodels\\nare\\nintegrated\\nwith\\nthe\\ntransformers\\nlibrary ,\\nmaking\\nit\\neasy\\nto\\nuse\\nand\\nfine-tune\\nthe\\nmodels\\nfor\\nspecific\\ntasks\\nUse\\nCases\\nand\\nBenchmarks\\nPaliGemma\\nis\\nsuitable\\nfor\\na\\nvariety\\nof\\ntasks,\\nincluding:\\n1.\\nImage\\nCaptioning:\\nPaliGemma\\ncan\\ngenerate\\ncaptions\\nfor\\nimages\\nbased\\non\\nthe\\ninput\\ntext\\nand\\nimage.\\n2.\\nV isual\\nQuestion\\nAnswering:\\nThe\\nmodel\\ncan\\nanswer\\nquestions\\nabout\\nimages,\\nproviding\\ndetailed\\nand\\ncontextual\\nresponses.\\n3.\\nT ext\\nReading\\nfrom\\nImages:\\nPaliGemma\\ncan\\nread\\ntext\\nembedded\\nwithin\\nimages,\\nsuch\\nas\\ncaptions\\nor\\nsigns.\\n4.\\nObject\\nDetection\\nand\\nSegmentation:\\nThe\\nmodel\\ncan\\nbe\\nfine-tuned\\nfor\\ntasks\\nlike\\nobject\\ndetection\\nand\\nsegmentation,\\nwhich\\ninvolve\\nidentifying\\nand\\nlocalizing\\nobjects\\nwithin\\nimages.\\nLimitations\\nand\\nFutur e\\nDirections\\n1.\\nNiche\\nDatasets:\\nPaliGemma\\nmay\\nstruggle\\nwith\\nniche\\ndatasets\\nor\\nenvironments\\nthat\\nwere\\nnot\\npresent\\nduring\\npretraining,\\nwhich\\nis\\nexpected\\ngiven\\nthe\\nlimited\\nscope\\nof\\nits\\npretraining.\\n2.\\nFine-T uning:\\nWhile\\nPaliGemma\\nis\\ndesigned\\nto\\nbe\\nfine-tuned,\\nthe\\nmodel's\\nperformance\\ncan\\nbe\\nimproved\\nsignificantly\\nby\\nfine-tuning\\nit\\non\\nspecific\\ntasks\\nand\\ndatasets.\\n3.\\nComparison\\nto\\nOther\\nModels:\\nPaliGemma\\ncan\\nbe\\ncompared\\nto\\nother\\nVLMs\\nand\\nLMMs,\\nsuch\\nas\\nChatGPT -4o,\\nwhich\\nhave\\nlar ger\\narchitectures\\nbut\\nmay\\nnot\\nbe\\nas\\nef ficient\\nor\\nfine-tunable\\nArchitectur e\\nof\\nPaligemma\\nThe\\narchitecture\\nof\\nPaligemma\\ncan\\nbe\\ndivided\\ninto\\nseveral\\nkey\\ncomponents:\\n1.\\nInput\\nProcessing\\nModule\\n:\\n○\\nVision\\nProcessing\\n:\\nThis\\nmodule\\nprocesses\\nvisual\\ninputs\\nusing\\nadvanced\\nvision\\nmodels\\nsuch\\nas\\nSigLIP .\\n○\\nLanguage\\nProcessing\\n:\\nThis\\nmodule\\nhandles\\ntextual\\ninputs\\nusing\\nthe\\nGemma\\nlanguage\\nmodel.\\n2.\\nMultimodal\\nFusion\\nLayer\\n:\", metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 1}),\n",
       " Document(page_content=\"○\\nThis\\nlayer\\nintegrates\\noutputs\\nfrom\\nboth\\nthe\\nvision\\nand\\nlanguage\\nprocessing\\nmodules\\nto\\ncreate\\na\\nunified\\nrepresentation.\\nT echniques\\nlike\\ncross-modal\\nattention\\nmechanisms\\nare\\noften\\nused\\nhere.\\n3.\\nCore\\nUnderstanding\\nEngine\\n:\\n○\\nContextual\\nUnderstanding\\n:\\nIntegrates\\nmultimodal\\ninformation\\nto\\nunderstand\\nthe\\ncontext\\nand\\nnuances\\nof\\nthe\\ninput\\ndata.\\n○\\nKnowledge\\nIntegration\\n:\\nUtilizes\\nexternal\\nknowledge\\nbases\\nto\\nenhance\\nunderstanding\\nand\\nprovide\\nmore\\naccurate\\nresponses.\\n4.\\nOutput\\nGeneration\\nModule\\n:\\n○\\nResponse\\nGeneration\\n:\\nUses\\nthe\\nintegrated\\nrepresentation\\nto\\ngenerate\\nappropriate\\nresponses\\nor\\nactions.\\n○\\nAdaptive\\nLearning\\n:\\nContinuously\\nlearns\\nfrom\\ninteractions\\nto\\nimprove\\nfuture\\nresponses.\\n5.\\nFeedback\\nLoop\\n:\\n○\\nPerformance\\nMonitoring\\n:\\nT racks\\nthe\\nperformance\\nof\\nthe\\nsystem\\nand\\nidentifies\\nareas\\nfor\\nimprovement.\\n○\\nIterative\\nLearning\\n:\\nUpdates\\nthe\\nmodel\\nbased\\non\\nfeedback\\nto\\nrefine\\nits\\ncapabilities.\\nPaliGemma\\nis\\na\\nvision-language\\nmodel\\n(VLM)\\ndeveloped\\nby\\nGoogle\\nthat\\ncombines\\na\\nvision\\nencoder\\nand\\na\\nlanguage\\ndecoder .\\nIts\\narchitecture\\nconsists\\nof:\\n●\\nSigLIP-400m\\nas\\nthe\\nvision\\nencoder:\\nSigLIP\\nis\\na\\nrobust\\ncontrastively\\ntrained\\nvisual\\nencoder\\nsimilar\\nto\\nOpenAI's\\nCLIP ,\\nbut\\nusing\\na\\nsimpler\\nsigmoid\\nloss\\nfunction.\\n●\\nGemma-2B\\nas\\nthe\\ntext\\ndecoder:\\nGemma\\nis\\na\\nrelatively\\ncompact\\ndecoder -only\\nlanguage\\nmodel\\nfrom\\nGoogle.\\nIt\\ntokenizes\\nthe\\ninput\\ntext\\nand\\nprocesses\\nall\\ntokens\\nusing\\nits\\n256,000\\ntoken\\nvocabulary .\\n●\\nGemma's\\ntransformer -based\\ndecoder:\\nThe\\ndecoder\\nis\\nlar gely\\nsimilar\\nto\\nthe\\noriginal\\ntransformer\\ndecoder\\nby\\nV aswani\\net\\nal.\\n(2017),\\nwith\\nmodifications\\nlike\\nmulti-head\\nattention,\\nrotary\\npositional\\nembeddings,\\nGeGLU\\nactivation,\\nand\\nRMSNorm.\\n●\\nAdditional\\ntokens:\\nPaliGemma\\nextends\\nGemma's\\ntoken\\nvocabulary\\nwith\\n1024\\nlocation\\ntokens\\n(<loc0000>\\nto\\n<loc1023>)\\nrepresenting\\nnormalized\\nimage\\ncoordinates,\\nand\\n128\\nsegmentation\\ntokens\\n(<seg000>\\nto\\n<seg127>)\\nfrom\\na\\nvector\\nquantized\\nvisual\\nauto-encoder .\\nThe\\nvision\\nencoder\\nand\\nlanguage\\ndecoder\\nare\\nlinked\\nusing\\na\\nmultimodal\\nlinear\\nprojection.\\nPaliGemma\\nis\\ndesigned\\nto\\ntake\\nboth\\nimage\\nand\\ntext\\nas\\ninput\\nand\\ngenerate\\ntext\\nas\\noutput,\\nsupporting\\nmultiple\\nlanguages.\\nThe\\nmodel\\nhas\\na\\ntotal\\nof\\n3\\nbillion\\nparameters\\nand\\nis\\npre-trained\\non\\na\\nmixture\\nof\\ndatasets\\nlike\\nW ebLI,\\nCC3M-35L,\\nVQ²A-CC3M-35L/VQG-CC3M-35L,\\nOpenImages,\\nand\\nWIT .\\nIt\\nis\\ndesigned\\nto\\nbe\\nfine-tuned\\non\\nspecific\\nvision-language\\ntasks\\nfor\\nbetter\\nperformance\", metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 2}),\n",
       " Document(page_content='How\\nPaligemma\\nWorks\\n1.\\nInput\\nReception\\n:\\nThe\\nsystem\\nreceives\\nvisual\\nand\\ntextual\\ninputs.\\n2.\\nProcessing\\n:\\nThe\\ninputs\\nare\\nprocessed\\nthrough\\ntheir\\nrespective\\nmodules—visual\\ndata\\nthrough\\nSigLIP\\nand\\ntextual\\ndata\\nthrough\\nGemma.\\n3.\\nIntegration\\n:\\nThe\\nmultimodal\\nfusion\\nlayer\\ncombines\\nthe\\nprocessed\\ndata\\ninto\\na\\ncoherent\\nrepresentation.\\n4.\\nUnderstanding\\n:\\nThe\\ncore\\nunderstanding\\nengine\\ninterprets\\nthe\\nintegrated\\ndata,\\nusing\\ncontext\\nand\\nexternal\\nknowledge.\\n5.\\nResponse\\nGeneration\\n:\\nAn\\nappropriate\\nresponse\\nis\\ngenerated\\nbased\\non\\nthe\\ninterpretation.\\n6.\\nLearning\\nand\\nAdaptation\\n:\\nThe\\nsystem\\nlearns\\nfrom\\ninteractions\\nand\\nfeedback\\nto\\nimprove\\nits\\nfuture\\nperformance.\\nSigLIP\\nVision\\nModel\\nDefinition\\nSigLIP\\n(Signal\\nLanguage-Image\\nPretraining)\\nis\\na\\nvision\\nmodel\\ndesigned\\nto\\nunderstand\\nand\\ninterpret\\nvisual\\ndata.\\nIt\\nemploys\\na\\npretraining\\ntechnique\\nthat\\nintegrates\\nboth\\nvisual\\nand\\ntextual\\ninformation\\nto\\nenhance\\nits\\nunderstanding\\ncapabilities.\\nSigLIP\\nis\\nbased\\non\\nthe\\nV ision\\nT ransformer\\n(V iT)\\narchitecture,\\nwhich\\nuses\\nself-attention\\nmechanisms\\nto\\nprocess\\ninput\\nimages.\\nThe\\nmodel\\nconsists\\nof\\na\\nseries\\nof\\ntransformer\\nblocks,\\neach\\nof\\nwhich\\nincludes\\na\\nmulti-head\\nself-attention\\nmechanism\\nand\\na\\nfeed-forward\\nnetwork\\n(FFN).\\nThe\\noutput\\nof\\neach\\nblock\\nis\\na\\nset\\nof\\nfeature\\nmaps\\nthat\\ncapture\\ndif ferent\\naspects\\nof\\nthe\\ninput\\nimage\\nArchitectur e\\nof\\nSigLIP\\n1.\\nImage\\nEncoder\\n:', metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 3}),\n",
       " Document(page_content='○\\nUtilizes\\nConvolutional\\nNeural\\nNetworks\\n(CNNs)\\nor\\nV ision\\nT ransformers\\n(V iT s)\\nto\\nextract\\nfeatures\\nfrom\\nimages.\\n○\\nMulti-layered\\nstructure\\nwith\\nattention\\nmechanisms\\nto\\nfocus\\non\\nimportant\\naspects\\nof\\nthe\\nvisual\\ndata.\\n2.\\nText\\nEncoder\\n:\\n○\\nIncorporates\\na\\nlanguage\\nmodel\\n(like\\nBER T\\nor\\nGPT)\\nto\\nprocess\\ntextual\\ndescriptions\\nassociated\\nwith\\nimages.\\n○\\nEmbedding\\nlayers\\nto\\nconvert\\ntext\\ninto\\nvector\\nrepresentations.\\n3.\\nCross-Modal\\nAttention\\nMechanism\\n:\\n○\\nConnects\\nthe\\nimage\\nand\\ntext\\nencoders,\\nallowing\\nthe\\nmodel\\nto\\nlearn\\ncorrespondences\\nbetween\\nvisual\\nfeatures\\nand\\ntextual\\ndescriptions.\\n○\\nUses\\nattention\\nlayers\\nto\\nhighlight\\nrelevant\\nparts\\nof\\nthe\\nimage\\nbased\\non\\nthe\\ntext\\nand\\nvice\\nversa.\\n4.\\nFusion\\nLayer\\n:\\n○\\nCombines\\nthe\\noutputs\\nof\\nthe\\nimage\\nand\\ntext\\nencoders\\ninto\\na\\nunified\\nrepresentation.\\n○\\nDense\\nlayers\\nand\\nnormalization\\ntechniques\\nto\\nensure\\ncohesive\\nintegration.\\n5.\\nOutput\\nLayer\\n:\\n○\\nProduces\\npredictions\\nor\\nclassifications\\nbased\\non\\nthe\\nfused\\nrepresentation.\\n○\\nCan\\nbe\\nfine-tuned\\nfor\\nspecific\\ntasks\\nsuch\\nas\\nimage\\ncaptioning,\\nvisual\\nquestion\\nanswering,\\nor\\nobject\\nrecognition.\\nFunctionality\\n●\\nPretraining\\n:\\nThe\\nmodel\\nis\\npretrained\\non\\nlar ge\\ndatasets\\ncontaining\\npaired\\nimage-text\\ndata\\nto\\nlearn\\nthe\\nrelationships\\nbetween\\nvisual\\nand\\ntextual\\ninformation.\\n●\\nFine-T uning\\n:\\nAfter\\npretraining,\\nthe\\nmodel\\ncan\\nbe\\nfine-tuned\\non\\nspecific\\ndatasets\\nto\\nadapt\\nto\\nvarious\\nvision-related\\ntasks.\\nTraining\\nSigLIP\\nis\\ntrained\\nusing\\na\\ncontrastive\\nloss\\nfunction,\\nwhich\\naims\\nto\\nmaximize\\nthe\\nsimilarity\\nbetween\\npositive\\npairs\\nof\\nimages\\nand\\nminimize\\nthe\\nsimilarity\\nbetween\\nnegative\\npairs.\\nThis\\napproach\\nhelps\\nthe\\nmodel\\nlearn\\nrobust\\nand\\ngeneralizable\\nrepresentations\\nof\\nimages.\\nThe\\nmodel\\nis\\ntrained\\non\\na\\nlar ge\\ndataset\\nof\\nimages\\nand\\ntheir\\ncorresponding\\ntext\\ndescriptions,\\nwhich\\nare\\nused\\nto\\ngenerate\\npositive\\nand\\nnegative\\npairs.', metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 4}),\n",
       " Document(page_content='Key\\nFeatur es\\n1.\\nRobustness:\\nSigLIP\\nis\\ndesigned\\nto\\nbe\\nrobust\\nto\\nvarious\\ntypes\\nof\\nimage\\ncorruptions\\nand\\ntransformations,\\nsuch\\nas\\nnoise,\\nblur ,\\nand\\nrotation.\\nThis\\nmakes\\nit\\nsuitable\\nfor\\nreal-world\\napplications\\nwhere\\nimages\\nmay\\nbe\\ndegraded\\nor\\ndistorted.\\n2.\\nEfficiency:\\nSigLIP\\nis\\noptimized\\nfor\\nef ficiency\\nand\\ncan\\nbe\\nused\\non\\na\\nwide\\nrange\\nof\\ndevices,\\nfrom\\nmobile\\nphones\\nto\\nhigh-performance\\nservers.\\nThis\\nmakes\\nit\\na\\npractical\\nchoice\\nfor\\nmany\\napplications.\\n3.\\nMultimodal\\nCapabilities:\\nSigLIP\\ncan\\nbe\\nused\\nin\\nconjunction\\nwith\\nother\\nmodels,\\nsuch\\nas\\nlanguage\\nmodels,\\nto\\nperform\\nmultimodal\\ntasks\\nlike\\nimage\\ncaptioning\\nand\\nvisual\\nquestion\\nanswering.\\nComparison\\nto\\nOther\\nModels\\nSigLIP\\nis\\ncomparable\\nto\\nother\\nrobust\\nvisual\\nencoders\\nlike\\nCLIP\\n,\\nwhich\\nis\\nalso\\ndeveloped\\nby\\nOpenAI.\\nWhile\\nboth\\nmodels\\nare\\ndesigned\\nto\\nbe\\nrobust\\nand\\nef ficient,\\nSigLIP\\nis\\nsimpler\\nand\\nmore\\nlightweight,\\nmaking\\nit\\neasier\\nto\\nintegrate\\ninto\\nvarious\\napplications.', metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 5}),\n",
       " Document(page_content='Implementation\\nSigLIP\\ncan\\nbe\\nimplemented\\nusing\\nthe\\ntransformers\\nlibrary\\nin\\nPython.\\nThe\\nfollowing\\ncode\\nsnippet\\ndemonstrates\\nhow\\nto\\nuse\\nSigLIP\\nfor\\nimage\\ncaptioning:\\nGemma\\nLanguage\\nModel\\nDefinition\\nThe\\nGemma\\nlanguage\\nmodel\\nis\\nan\\nadvanced\\nneural\\nnetwork\\nmodel\\ndesigned\\nto\\nunderstand\\nand\\ngenerate\\nhuman\\nlanguage.\\nIt\\nleverages\\nextensive\\npretraining\\non\\nvast\\namounts\\nof\\ntext\\ndata\\nto\\ndevelop\\na\\ndeep\\nunderstanding\\nof\\nlanguage\\nnuances.\\nGemma\\nis\\na\\nfamily\\nof\\nlightweight,\\nstate-of-the-art\\nopen\\nmodels\\ndeveloped\\nby\\nGoogle.\\nIt\\nis\\ndesigned\\nto\\nbe\\na\\nrobust\\nand\\nef ficient\\nmodel\\nthat\\ncan\\nbe\\nused\\nfor\\nvarious\\napplications\\nsuch\\nas\\ntext\\ngeneration\\nand\\nmultimodal\\ntasks.\\nHere\\nare\\nthe\\nkey\\naspects\\nof\\nGemma:\\nArchitectur e\\nand\\nTraining\\nGemma\\nis\\nbased\\non\\nthe\\ntransformer\\narchitecture\\nand\\nis\\ntrained\\nusing\\na\\ncombination\\nof\\nmasked\\nlanguage\\nmodeling\\nand\\nnext\\nsentence\\nprediction\\ntasks.\\nThe\\nmodel\\nis\\ntrained\\non\\na\\nlar ge\\ndataset\\nof\\ntext\\nand\\nis\\ndesigned\\nto\\nbe\\nrobust\\nand\\nef ficient.\\nModel\\nSizes\\nand\\nCapabilities\\nGemma\\nmodels\\nare\\navailable\\nin\\ntwo\\nsizes:\\n2B\\nand\\n7B.\\nThe\\n2B\\nmodel\\nis\\ndesigned\\nfor\\nlower\\nresource\\nrequirements\\nand\\ncan\\nrun\\non\\nmobile\\ndevices\\nand\\nlaptops,\\nwhile\\nthe\\n7B\\nmodel\\nis\\nmore\\npowerful\\nand\\ncan\\nrun\\non\\ndesktop\\ncomputers\\nand\\nsmall\\nservers.\\nTuning\\nand\\nCustomization', metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 6}),\n",
       " Document(page_content='Gemma\\nmodels\\ncan\\nbe\\ntuned\\nand\\ncustomized\\nfor\\nspecific\\ntasks\\nusing\\ntechniques\\nsuch\\nas\\nLoRA\\n(Low-Rank\\nAdaptation)\\nand\\nmodel\\nparallelism.\\nThis\\nallows\\ndevelopers\\nto\\nadapt\\nthe\\nmodel\\nto\\ntheir\\nspecific\\nneeds\\nand\\nimprove\\nits\\nperformance\\non\\ntar geted\\ntasks.\\nResponsible\\nAI\\nDevelopment\\nGemma\\nis\\ndesigned\\nwith\\nresponsible\\nAI\\ndevelopment\\nin\\nmind.\\nThe\\nmodel\\nis\\ntrained\\non\\ncurated\\ndata\\nand\\nis\\ntuned\\nfor\\nsafety\\nusing\\ntechniques\\nsuch\\nas\\nautomated\\nfiltering\\nof\\npersonal\\ninformation\\nand\\nextensive\\nfine-tuning\\nwith\\nhuman\\nfeedback.\\nThe\\nmodel\\nis\\nalso\\nevaluated\\nusing\\nrobust\\nmethods\\nsuch\\nas\\nmanual\\nred-teaming\\nand\\nautomated\\nadversarial\\ntesting\\nto\\nensure\\nit\\ndoes\\nnot\\nexhibit\\ndangerous\\nbehaviors.\\nDeployment\\nand\\nIntegration\\nGemma\\nmodels\\ncan\\nbe\\ndeployed\\non\\nvarious\\nplatforms,\\nincluding\\nGoogle\\nCloud,\\nand\\ncan\\nbe\\nintegrated\\nwith\\npopular\\nframeworks\\nsuch\\nas\\nJAX,\\nPyT orch,\\nand\\nT ensorFlow .\\nThe\\nmodel\\ncan\\nalso\\nbe\\nfine-tuned\\non\\nspecific\\ndata\\nsets\\nand\\ntasks\\nusing\\ntools\\nsuch\\nas\\nLoRA\\nand\\nmodel\\nparallelism.\\nPerformance\\nand\\nBenchmarks\\nGemma\\nmodels\\nhave\\nachieved\\nstate-of-the-art\\nperformance\\nfor\\ntheir\\nsize\\ncompared\\nto\\nother\\nopen\\nmodels.\\nThe\\nmodel\\nhas\\nbeen\\ntested\\non\\nvarious\\nbenchmarks\\nand\\nhas\\nshown\\nexceptional\\nperformance\\nin\\ntasks\\nsuch\\nas\\ntext\\ngeneration\\nand\\nmultimodal\\ntasks.\\nAvailability\\nand\\nCommunity\\nGemma\\nmodels\\nare\\navailable\\nfor\\ndownload\\nfrom\\nKaggle\\nand\\ncan\\nbe\\nused\\nfor\\nvarious\\napplications.\\nThe\\nmodel\\nhas\\na\\ngrowing\\ncommunity\\nof\\ndevelopers\\nand\\nresearchers\\nwho\\nare\\nworking\\non\\nfine-tuning\\nand\\ncustomizing\\nthe\\nmodel\\nfor\\nspecific\\ntasks.\\nKey\\nFeatur es\\n1.\\nLightweight\\nand\\nEf ficient:\\nGemma\\nmodels\\nare\\ndesigned\\nto\\nbe\\nlightweight\\nand\\nef ficient,\\nmaking\\nthem\\nsuitable\\nfor\\ndeployment\\non\\na\\nwide\\nrange\\nof\\ndevices\\nand\\nplatforms.\\n2.\\nState-of-the-Art\\nPerformance:\\nGemma\\nmodels\\nhave\\nachieved\\nstate-of-the-art\\nperformance\\nfor\\ntheir\\nsize\\ncompared\\nto\\nother\\nopen\\nmodels.\\n3.\\nResponsible\\nAI\\nDevelopment:\\nGemma\\nis\\ndesigned\\nwith\\nresponsible\\nAI\\ndevelopment\\nin\\nmind,\\nincorporating\\ntechniques\\nsuch\\nas\\nautomated\\nfiltering\\nof\\npersonal\\ninformation\\nand\\nextensive\\nfine-tuning\\nwith\\nhuman\\nfeedback.\\n4.\\nCustomization\\nand\\nT uning:\\nGemma\\nmodels\\ncan\\nbe\\ntuned\\nand\\ncustomized\\nfor\\nspecific\\ntasks\\nusing\\ntechniques\\nsuch\\nas\\nLoRA\\nand\\nmodel\\nparallelism.\\n5.\\nIntegration\\nwith\\nPopular\\nFrameworks:\\nGemma\\nmodels\\ncan\\nbe\\nintegrated\\nwith\\npopular\\nframeworks\\nsuch\\nas\\nJAX,\\nPyT orch,\\nand\\nT ensorFlow ,\\nmaking\\nit\\neasy\\nto\\nuse\\nand\\ndeploy', metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 7}),\n",
       " Document(page_content='Architectur e\\nof\\nGemma\\n1.\\nEmbedding\\nLayer\\n:\\n○\\nConverts\\ninput\\ntext\\ninto\\ndense\\nvector\\nrepresentations.\\n○\\nUtilizes\\nword\\nembeddings\\nor\\ncontextual\\nembeddings\\nlike\\nthose\\nfrom\\nBER T\\nor\\nGPT .\\n2.\\nTransformer\\nLayers\\n:\\n○\\nMultiple\\ntransformer\\nblocks,\\neach\\ncontaining\\nself-attention\\nmechanisms\\nand\\nfeed-forward\\nneural\\nnetworks.\\n○\\nLayer\\nnormalization\\nand\\nresidual\\nconnections\\nto\\nmaintain\\nstable\\ntraining.\\n3.\\nContextual\\nUnderstanding\\nModule\\n:\\n○\\nEncodes\\nthe\\ncontext\\nof\\nthe\\ninput\\ntext\\nto\\ngenerate\\ncoherent\\nand\\ncontextually\\nappropriate\\nresponses.\\n○\\nUses\\nattention\\nmechanisms\\nto\\nfocus\\non\\nrelevant\\nparts\\nof\\nthe\\ninput\\ntext.\\n4.\\nOutput\\nLayer\\n:\\n○\\nGenerates\\nthe\\nfinal\\noutput\\ntext\\nbased\\non\\nthe\\nprocessed\\nand\\ncontextually\\nunderstood\\ninput.\\n○\\nCan\\nproduce\\nvarious\\nforms\\nof\\noutput\\nsuch\\nas\\nsummaries,\\ntranslations,\\nor\\nconversational\\nresponses.\\nFunctionality\\n●\\nPretraining\\n:\\nT rained\\non\\ndiverse\\ntext\\ncorpora\\nto\\nlearn\\nlanguage\\npatterns,\\ngrammar ,\\nand\\ncontext.\\n●\\nFine-T uning\\n:\\nAdapted\\nto\\nspecific\\ntasks\\nlike\\nquestion\\nanswering,\\ntext\\ngeneration,\\nor\\nsentiment\\nanalysis\\nthrough\\nfine-tuning\\non\\nrelevant\\ndatasets.\\nIntegration\\nin\\nPaligemma\\nIn\\nthe\\nPaligemma\\nsystem,\\nSigLIP\\nand\\nGemma\\nwork\\nin\\ntandem\\nto\\nprocess\\nand\\nunderstand\\nmultimodal\\ninputs.\\nSigLIP\\nhandles\\nvisual\\ndata,\\nconverting\\nit\\ninto\\na\\nformat\\nthat\\ncan\\nbe\\nintegrated\\nwith\\nthe\\ntextual\\ndata\\nprocessed\\nby\\nGemma.\\nThe\\nfusion\\nlayer\\nthen\\ncombines\\nthese\\nrepresentations,\\nallowing\\nthe\\ncore\\nunderstanding\\nengine\\nto\\ninterpret\\nthe\\nintegrated\\ndata\\nand\\ngenerate\\nappropriate\\nresponses.\\nThis\\nintegration\\nenables\\nPaligemma\\nto\\nexcel\\nin\\ntasks\\nthat\\nrequire\\na\\ndeep\\nunderstanding\\nof\\nboth\\nvisual\\nand\\ntextual\\ninformation.\\nModel\\nData\\nData\\nfor\\nPaligemma\\nPaligemma,\\nbeing\\na\\nmultimodal\\nsystem,\\nrequires\\ndiverse\\ndatasets\\nencompassing\\nboth\\nvisual\\nand\\ntextual\\ndata:', metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 8}),\n",
       " Document(page_content=\"1.\\nImage-T ext\\nPairs\\n:\\nDatasets\\nlike\\nCOCO\\n(Common\\nObjects\\nin\\nContext)\\nand\\nV isual\\nGenome\\nprovide\\nimages\\nwith\\ncorresponding\\ntextual\\ndescriptions\\nor\\nannotations.\\n2.\\nTextual\\nData\\n:\\nLar ge\\ncorpora\\nof\\ntext,\\nsuch\\nas\\nW ikipedia\\narticles,\\nbooks,\\nand\\nweb\\npages,\\nare\\nused\\nto\\npretrain\\nthe\\nlanguage\\nmodel\\n(Gemma).\\n3.\\nAnnotated\\nMultimodal\\nData\\n:\\nDatasets\\nspecifically\\ndesigned\\nfor\\ntasks\\nlike\\nvisual\\nquestion\\nanswering\\n(VQA),\\nimage\\ncaptioning,\\nand\\nscene\\nunderstanding,\\nwhich\\ncombine\\nboth\\nimage\\nand\\ntext\\nannotations.\\nData\\nfor\\nSigLIP\\nSigLIP's\\ntraining\\ndata\\nincludes:\\n1.\\nImage\\nDatasets\\n:\\nHigh-quality\\nand\\ndiverse\\nimage\\ndatasets\\nlike\\nImageNet,\\nCOCO,\\nand\\nOpen\\nImages.\\n2.\\nPaired\\nImage-T ext\\nData\\n:\\nData\\nwhere\\neach\\nimage\\nis\\npaired\\nwith\\ndescriptive\\ntext,\\naiding\\nthe\\nmodel\\nin\\nlearning\\nthe\\nrelationships\\nbetween\\nvisual\\ncontent\\nand\\nlanguage.\\nData\\nfor\\nGemma\\nGemma's\\ndata\\nrequirements\\nare\\nprimarily\\ntextual:\\n1.\\nLarge-Scale\\nText\\nCorpora\\n:\\nDatasets\\nsuch\\nas\\nthe\\nCommon\\nCrawl,\\nW ikipedia,\\nand\\nBookCorpus\\nprovide\\nextensive\\ntext\\ndata\\nfor\\npretraining.\\n2.\\nSpecialized\\nText\\nDatasets\\n:\\nFine-tuning\\ndatasets\\ntailored\\nto\\nspecific\\ntasks\\nlike\\nsentiment\\nanalysis,\\nquestion\\nanswering\\n(SQuAD),\\nand\\nnatural\\nlanguage\\ninference.\\nModel\\nBuilding\\nBuilding\\nPaligemma\\n1.\\nData\\nCollection\\nand\\nPreprocessing\\n:\\n○\\nCollect\\nand\\nclean\\nlar ge\\nvolumes\\nof\\nmultimodal\\ndata.\\n○\\nPreprocess\\nimages\\n(resizing,\\nnormalization)\\nand\\ntext\\n(tokenization,\\nnormalization).\\n2.\\nPretraining\\n:\\n○\\nT rain\\nSigLIP\\non\\nimage-text\\npairs\\nto\\nlearn\\ncross-modal\\nrepresentations.\\n○\\nT rain\\nGemma\\non\\nlar ge-scale\\ntext\\ncorpora\\nto\\ndevelop\\na\\ndeep\\nunderstanding\\nof\\nlanguage.\\n3.\\nMultimodal\\nFusion\\n:\\n○\\nDevelop\\nand\\ntrain\\nthe\\nfusion\\nlayer\\nto\\nintegrate\\nvisual\\nand\\ntextual\\nfeatures.\\n○\\nUtilize\\ntechniques\\nlike\\ncross-attention\\nto\\nef fectively\\ncombine\\nmodalities.\\n4.\\nFine-T uning\\n:\\n○\\nFine-tune\\nthe\\nintegrated\\nmodel\\non\\ntask-specific\\nmultimodal\\ndatasets.\\n○\\nUse\\ntransfer\\nlearning\\nto\\nadapt\\npre-trained\\nmodels\\nto\\nnew\\ntasks\\nwith\\nlimited\\ndata.\", metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 9}),\n",
       " Document(page_content='Building\\nSigLIP\\n1.\\nImage\\nEncoder\\nTraining\\n:\\n○\\nUse\\nconvolutional\\nneural\\nnetworks\\n(CNNs)\\nor\\nV ision\\nT ransformers\\n(V iT s)\\nto\\nextract\\nfeatures\\nfrom\\nimages.\\n○\\nT rain\\non\\nlar ge\\nimage\\ndatasets\\nto\\ndevelop\\nrobust\\nvisual\\nrepresentations.\\n2.\\nText\\nEncoder\\nTraining\\n:\\n○\\nEmploy\\ntransformers\\nto\\nprocess\\ntextual\\ndescriptions\\nassociated\\nwith\\nimages.\\n○\\nT rain\\non\\npaired\\nimage-text\\ndata\\nto\\nlearn\\nthe\\ncorrelation\\nbetween\\nvisual\\nand\\ntextual\\ninformation.\\n3.\\nCross-Modal\\nPretraining\\n:\\n○\\nImplement\\ncross-modal\\nattention\\nmechanisms\\nto\\nalign\\nimage\\nfeatures\\nwith\\ntextual\\nfeatures.\\n○\\nPretrain\\non\\nlar ge-scale\\ndatasets\\nto\\nlearn\\nrich,\\nshared\\nrepresentations.\\nBuilding\\nGemma\\n1.\\nEmbedding\\nLayer\\nSetup\\n:\\n○\\nInitialize\\nword\\nembeddings\\nusing\\npre-trained\\nvectors\\nor\\ntrain\\nembeddings\\nfrom\\nscratch\\non\\na\\nlar ge\\ntext\\ncorpus.\\n2.\\nTransformer\\nTraining\\n:\\n○\\nUse\\ntransformer\\narchitecture\\nwith\\nmultiple\\nlayers\\nof\\nself-attention\\nand\\nfeed-forward\\nnetworks.\\n○\\nPretrain\\non\\nextensive\\ntext\\ndata\\nto\\ncapture\\nlanguage\\npatterns\\nand\\ncontextual\\nrelationships.\\n3.\\nContextual\\nUnderstanding\\n:\\n○\\nIntegrate\\nadvanced\\nattention\\nmechanisms\\nto\\nfocus\\non\\nrelevant\\ntext\\nsegments.\\n○\\nT rain\\non\\ndatasets\\nlike\\nbooks,\\narticles,\\nand\\ndialogues\\nto\\nunderstand\\nvarious\\ncontexts\\nand\\nnuances.\\nModel\\nArchitectur e\\nArchitectur e\\nof\\nPaligemma\\n1.\\nInput\\nProcessing\\nModule\\n:\\n○\\nSigLIP\\nVision\\nProcessing\\n:\\nImage\\nencoder\\n+\\ntext\\nencoder\\n+\\ncross-modal\\nattention.\\n○\\nGemma\\nLanguage\\nProcessing\\n:\\nT ransformer -based\\nlanguage\\nmodel.\\n2.\\nMultimodal\\nFusion\\nLayer\\n:\\n○\\nCross-attention\\nmechanisms\\nto\\ncombine\\nimage\\nand\\ntext\\nfeatures.\\n○\\nDense\\nlayers\\nand\\nnormalization\\nto\\ncreate\\na\\nunified\\nrepresentation.\\n3.\\nCore\\nUnderstanding\\nEngine\\n:\\n○\\nContextual\\nunderstanding\\nthrough\\nintegrated\\nrepresentations.', metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 10}),\n",
       " Document(page_content='○\\nExternal\\nknowledge\\nintegration\\nvia\\nAPIs\\nor\\ndatabases\\nto\\nenhance\\ncomprehension.\\n4.\\nOutput\\nGeneration\\nModule\\n:\\n○\\nDecoders\\nfor\\ngenerating\\ntextual\\nresponses\\nor\\nactions.\\n○\\nT ask-specific\\nlayers\\nfor\\napplications\\nlike\\nVQA\\nor\\nimage\\ncaptioning.\\n5.\\nFeedback\\nLoop\\n:\\n○\\nPerformance\\nmonitoring\\nto\\nidentify\\nstrengths\\nand\\nweaknesses.\\n○\\nIterative\\nlearning\\nto\\nadapt\\nand\\nimprove\\nbased\\non\\nfeedback.\\nArchitectur e\\nof\\nSigLIP\\n1.\\nImage\\nEncoder\\n:\\n○\\nConvolutional\\nlayers\\nor\\nV ision\\nT ransformer\\nlayers\\nto\\nprocess\\nimages.\\n○\\nAttention\\nmechanisms\\nto\\nfocus\\non\\nimportant\\nvisual\\nfeatures.\\n2.\\nText\\nEncoder\\n:\\n○\\nT ransformer\\nlayers\\nto\\nprocess\\nassociated\\ntextual\\ndescriptions.\\n○\\nEmbedding\\nlayers\\nto\\nconvert\\ntext\\ninto\\nmeaningful\\nvectors.\\n3.\\nCross-Modal\\nAttention\\nMechanism\\n:\\n○\\nAttention\\nlayers\\nconnecting\\nimage\\nand\\ntext\\nencoders.\\n○\\nMechanisms\\nto\\nhighlight\\nrelevant\\nimage\\nregions\\nbased\\non\\ntext\\nand\\nvice\\nversa.\\n4.\\nFusion\\nLayer\\n:\\n○\\nCombines\\nimage\\nand\\ntext\\nembeddings\\ninto\\na\\ncohesive\\nrepresentation.\\n○\\nDense\\nlayers\\nfor\\nintegration\\nand\\nnormalization.\\n5.\\nOutput\\nLayer\\n:\\n○\\nT ask-specific\\nlayers\\nfor\\nclassification,\\ncaptioning,\\nor\\nother\\nvision\\ntasks.\\n○\\nFine-tuning\\nmechanisms\\nto\\nadapt\\nto\\ndif ferent\\napplications.\\nArchitectur e\\nof\\nGemma\\n1.\\nEmbedding\\nLayer\\n:\\n○\\nConverts\\ntext\\ninto\\ndense\\nvector\\nrepresentations.\\n○\\nUtilizes\\npre-trained\\nword\\nembeddings\\nor\\ncontextual\\nembeddings.\\n2.\\nTransformer\\nLayers\\n:\\n○\\nMultiple\\nlayers\\nof\\nself-attention\\nand\\nfeed-forward\\nnetworks.\\n○\\nLayer\\nnormalization\\nand\\nresidual\\nconnections\\nfor\\nstable\\ntraining.\\n3.\\nContextual\\nUnderstanding\\nModule\\n:\\n○\\nAttention\\nmechanisms\\nto\\nfocus\\non\\nrelevant\\nparts\\nof\\nthe\\ntext.\\n○\\nMechanisms\\nto\\nmaintain\\ncontext\\nover\\nlong\\ntext\\nsequences.\\n4.\\nOutput\\nLayer\\n:\\n○\\nGenerates\\nfinal\\ntext\\noutputs,\\nsuch\\nas\\nresponses,\\nsummaries,\\nor\\ntranslations.\\n○\\nT ask-specific\\nadaptations\\nfor\\ndif ferent\\nlanguage\\napplications.', metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 11}),\n",
       " Document(page_content=\"By\\nintegrating\\nthese\\nsophisticated\\ncomponents,\\nPaligemma\\nachieves\\na\\npowerful\\nsyner gy\\nbetween\\nvision\\nand\\nlanguage,\\nenabling\\nit\\nto\\nperform\\ncomplex\\ntasks\\nthat\\nrequire\\ndeep\\nunderstanding\\nand\\ngeneration\\ncapabilities\\nacross\\nboth\\nmodalities.\\nFine-tuning\\nPaligemma\\ninvolves\\nadjusting\\nthe\\npre-trained\\nmodels\\n(SigLIP\\nfor\\nvision\\nand\\nGemma\\nfor\\nlanguage)\\non\\na\\nspecific\\ntask\\nusing\\na\\ntailored\\ndataset.\\nHere's\\na\\ndetailed\\nguide\\non\\nhow\\nto\\nfine-tune\\nPaligemma,\\nalong\\nwith\\ncode\\nexamples.\\nPrerequisites\\n1.\\nData\\nPreparation\\n:\\nY ou\\nneed\\na\\ndataset\\nrelevant\\nto\\nyour\\ntask,\\nwhich\\ncontains\\nboth\\nvisual\\nand\\ntextual\\ninformation.\\nFor\\nexample,\\na\\nV isual\\nQuestion\\nAnswering\\n(VQA)\\ndataset.\\n2.\\nEnvir onment\\nSetup\\n:\\nEnsure\\nyou\\nhave\\na\\nsuitable\\nmachine\\nlearning\\nenvironment\\nwith\\nnecessary\\nlibraries\\ninstalled\\n(e.g.,\\nPyT orch,\\nT ransformers,\\nV ision\\nlibraries).\\nStep-by-Step\\nFine-T uning\\nProcess\\nPaliGemma\\nis\\na\\nvision-language\\nmodel\\n(VLM)\\ndeveloped\\nby\\nGoogle\\nthat\\ncan\\nbe\\nfine-tuned\\nfor\\nvarious\\ntasks.\\nFine-tuning\\ninvolves\\nadjusting\\nthe\\nmodel's\\nweights\\nbased\\non\\nspecific\\ntask\\nrequirements\\nand\\ndatasets\\nto\\nimprove\\nits\\nperformance.\\nHere's\\na\\nstep-by-step\\nguide\\non\\nhow\\nto\\nfine-tune\\nPaliGemma:\\nStep\\n1:\\nDownload\\nthe\\nModel\\nand\\nDependencies\\n1.\\nDownload\\nPaliGemma\\nModel\\nCheckpoint:\\nDownload\\nthe\\npre-trained\\nPaliGemma\\nmodel\\ncheckpoint\\nand\\ntokenizer\\nfrom\\nKaggle\\nor\\nother\\nsources.\\n2.\\nInstall\\nDependencies:\\nInstall\\nthe\\nrequired\\ndependencies,\\nincluding\\nJAX,\\nT ensorFlow ,\\nNumPy ,\\nand\\nother\\nlibraries.\\nStep\\n2:\\nPrepare\\nthe\\nModel\\n1.\\nLoad\\nthe\\nModel:\\nLoad\\nthe\\npre-trained\\nPaliGemma\\nmodel\\nonto\\nGPU\\ndevices.\\n2.\\nPrepare\\nInputs:\\nPrepare\\nthe\\nmodel's\\ninputs\\nfor\\ntraining\\nand\\ninference\\nby\\nprocessing\\nthe\\ndata\\ninto\\nthe\\nrequired\\nformat.\\nStep\\n3:\\nFine-tune\\nthe\\nModel\\n1.\\nFreeze\\nParameters:\\nFreeze\\nthe\\nmajority\\nof\\nthe\\nmodel's\\nparameters,\\nexcept\\nfor\\nthe\\nattention\\nlayers,\\nto\\nprevent\\noverfitting.\\n2.\\nT rain\\nthe\\nModel:\\nT rain\\nthe\\nfine-tuned\\nmodel\\nusing\\nthe\\nspecific\\ntask's\\ndataset\\nand\\nhyperparameters.\\nThis\\nstep\\ncan\\nbe\\ndone\\nusing\\nJAX\\nand\\nother\\nlibraries.\\nStep\\n4:\\nTest\\nand\\nSave\\nthe\\nModel\\n1.\\nT est\\nthe\\nModel:\\nT est\\nthe\\nfine-tuned\\nmodel\\non\\na\\nvalidation\\ndataset\\nto\\nevaluate\\nits\\nperformance.\", metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 12}),\n",
       " Document(page_content='2.\\nSave\\nthe\\nModel:\\nSave\\nthe\\nfine-tuned\\nmodel\\nfor\\nlater\\nuse\\nby\\nsaving\\nits\\nweights\\nand\\nother\\nparameters.\\nAdditional\\nTips\\n1.\\nUse\\na\\nSmall\\nV ersion:\\nFor\\nfine-tuning\\nin\\nGoogle\\nColab,\\nuse\\nthe\\nsmallest\\nversion\\nof\\nPaliGemma\\n(paligemma-3b-pt-224)\\nto\\nlimit\\nGPU\\nmemory\\nconsumption.\\n2.\\nUse\\nRoboflow\\nUniverse:\\nUse\\nRoboflow\\nUniverse\\nfor\\naccessing\\nand\\nmanaging\\ndatasets\\nfor\\nfine-tuning\\nPaliGemma.\\n3.\\nFine-tune\\nfor\\nSpecific\\nT asks:\\nFine-tune\\nPaliGemma\\nfor\\nspecific\\ntasks\\nsuch\\nas\\nobject\\ndetection,\\nsegmentation,\\nor\\ntext\\nreading\\nfrom\\nimages\\n1.\\nImport\\nNecessary\\nLibraries\\n2.\\nPrepar e\\nthe\\nDataset\\nDefine\\na\\ncustom\\ndataset\\nclass\\nto\\nhandle\\nyour\\ndata.\\n3.\\nInitialize\\nPre-trained\\nModels\\nLoad\\npre-trained\\nmodels\\nfor\\nvision\\nand\\nlanguage.', metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 13}),\n",
       " Document(page_content='4.\\nDefine\\nthe\\nMultimodal\\nModel\\nCombine\\nvision\\nand\\nlanguage\\nmodels\\ninto\\na\\nsingle\\narchitecture.\\n5.\\nPrepar e\\nfor\\nTraining\\nSet\\nup\\nthe\\ndata\\nloaders,\\nloss\\nfunction,\\nand\\noptimizer .\\n6.\\nFine-T uning\\nthe\\nModel\\nDefine\\nthe\\ntraining\\nloop\\nto\\nfine-tune\\nthe\\nmodel.', metadata={'source': 'Finetuning PaliGemma.pdf', 'page': 14})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paligemma\n",
      ":\n",
      "Paligemma\n",
      "is\n",
      "a\n",
      "sophisticated,\n",
      "integrated\n",
      "AI\n",
      "system\n",
      "that\n",
      "combines\n",
      "vision\n",
      "and\n",
      "language\n",
      "models\n",
      "to\n",
      "provide\n",
      "comprehensive\n",
      "multimodal\n",
      "understanding\n",
      "and\n",
      "generation\n",
      "capabilities.\n",
      "The\n",
      "name\n",
      "\"Paligemma\"\n",
      "suggests\n",
      "a\n",
      "combination\n",
      "of\n",
      "\"Pali,\"\n",
      "potentially\n",
      "hinting\n",
      "at\n",
      "a\n",
      "foundational\n",
      "or\n",
      "structural\n",
      "aspect,\n",
      "and\n",
      "\"gemma,\"\n",
      "which\n",
      "can\n",
      "imply\n",
      "something\n",
      "valuable\n",
      "or\n",
      "precious,\n",
      "indicating\n",
      "the\n",
      "integration\n",
      "of\n",
      "crucial\n",
      "AI\n",
      "components.\n",
      "PaliGemma\n",
      "is\n",
      "a\n",
      "vision-language\n",
      "model\n",
      "(VLM)\n",
      "developed\n",
      "by\n",
      "Google.\n",
      "It\n",
      "is\n",
      "a\n",
      "multimodal\n",
      "model\n",
      "that\n",
      "combines\n",
      "the\n",
      "capabilities\n",
      "of\n",
      "a\n",
      "vision\n",
      "model\n",
      "and\n",
      "a\n",
      "language\n",
      "model.\n",
      "The\n",
      "model\n",
      "is\n",
      "composed\n",
      "of\n",
      "a\n",
      "Siglip-400m\n",
      "vision\n",
      "encoder\n",
      "and\n",
      "a\n",
      "Gemma-2B\n",
      "decoder\n",
      "linked\n",
      "by\n",
      "a\n",
      "multimodal\n",
      "linear\n",
      "projection.\n",
      "PaliGemma\n",
      "is\n",
      "designed\n",
      "to\n",
      "process\n",
      "both\n",
      "images\n",
      "and\n",
      "text\n",
      "and\n",
      "generate\n",
      "text\n",
      "as\n",
      "output,\n",
      "supporting\n",
      "multiple\n",
      "languages\n",
      "Key\n",
      "Featur es\n",
      "and\n",
      "Capabilities\n",
      "1.\n",
      "Multimodal\n",
      "Comprehension:\n",
      "PaliGemma\n",
      "can\n",
      "simultaneously\n",
      "understand\n",
      "both\n",
      "images\n",
      "and\n",
      "text,\n",
      "making\n",
      "it\n",
      "suitable\n",
      "for\n",
      "tasks\n",
      "such\n",
      "as\n",
      "image\n",
      "captioning,\n",
      "visual\n",
      "question\n",
      "answering,\n",
      "and\n",
      "text\n",
      "reading\n",
      "from\n",
      "images.\n",
      "2.\n",
      "Fine-T uning:\n",
      "PaliGemma\n",
      "is\n",
      "designed\n",
      "to\n",
      "be\n",
      "fine-tuned\n",
      "on\n",
      "specific\n",
      "tasks,\n",
      "which\n",
      "allows\n",
      "it\n",
      "to\n",
      "adapt\n",
      "to\n",
      "dif ferent\n",
      "use\n",
      "cases\n",
      "and\n",
      "achieve\n",
      "better\n",
      "performance.\n",
      "This\n",
      "fine-tuning\n",
      "process\n",
      "involves\n",
      "adjusting\n",
      "the\n",
      "model's\n",
      "weights\n",
      "based\n",
      "on\n",
      "the\n",
      "specific\n",
      "task\n",
      "and\n",
      "dataset.\n",
      "3.\n",
      "Pre-T raining:\n",
      "PaliGemma\n",
      "is\n",
      "pre-trained\n",
      "on\n",
      "a\n",
      "variety\n",
      "of\n",
      "datasets,\n",
      "including\n",
      "W ebLI,\n",
      "CC3M-35L,\n",
      "VQ²A-CC3M-35L/VQG-CC3M-35L,\n",
      "OpenImages,\n",
      "and\n",
      "WIT .\n",
      "This\n",
      "pre-training\n",
      "helps\n",
      "the\n",
      "model\n",
      "learn\n",
      "general\n",
      "representations\n",
      "of\n",
      "images\n",
      "and\n",
      "text\n",
      "that\n",
      "can\n",
      "be\n",
      "leveraged\n",
      "for\n",
      "downstream\n",
      "tasks.\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Online PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders  import OnlinePDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader  = OnlinePDFLoader('https://arxiv.org/pdf/1802.00308.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
